{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "denseNet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mnansary/pyF2O/blob/master/colab_gen_unet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ojVYZ7Spzpv",
        "colab_type": "text"
      },
      "source": [
        "# colab specific task\n",
        "*   mount google drive\n",
        "*   change working directory to git repo\n",
        "*   TPU check\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOI03Qu0tHrf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install tensorflow==1.13.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--q4JaV2ps6z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsVhQoAOqGGW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cd /content/gdrive/My\\ Drive/PROJECTS/MED/pySKIND"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "692-uM-yqdaF",
        "colab_type": "text"
      },
      "source": [
        "## TPU Check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JT9QkSiJqed6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pprint\n",
        "import tensorflow as tf\n",
        "\n",
        "if 'COLAB_TPU_ADDR' not in os.environ:\n",
        "  print('ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!')\n",
        "else:\n",
        "  tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "  print ('TPU address is', tpu_address)\n",
        "\n",
        "  with tf.Session(tpu_address) as session:\n",
        "    devices = session.list_devices()\n",
        "    \n",
        "  print('TPU devices:')\n",
        "  pprint.pprint(devices)\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxefiHZ4qlHA",
        "colab_type": "text"
      },
      "source": [
        "# DenseNet Model Training\n",
        "* Define **Parameters for training**\n",
        "* Define **FLAGS for DenseNet**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2usCWY1Fc6E",
        "colab_type": "text"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tdzez_PiFVTM",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "845e814e-a582-4979-c9a2-9be12e6fb485",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "import sys\n",
        "sys.path.append('.')\n",
        "\n",
        "from coreLib.utils import readh5\n",
        "import numpy as np \n",
        "class PARAMS:\n",
        "    DS_DIR  = '/content/gdrive/My Drive/PROJECTS/MED/Train/' # @param\n",
        "    BATCH_SIZE      = 128  # @param\n",
        "    NUM_EPOCHS      = 100  # @param\n",
        "    X_TRAIN_IDEN    = 'X_train.h5'  # @param\n",
        "    Y_TRAIN_IDEN    = 'Y_train.h5'  # @param\n",
        "    X_EVAL_IDEN     = 'X_eval.h5'  # @param\n",
        "    Y_EVAL_IDEN     = 'Y_eval.h5'  # @param\n",
        "    MODEL_DIR       = '/content/gdrive/My Drive/PROJECTS/MED/MODEL_DIR/' # @param\n",
        "    MODEL_NAME      = 'denseNet' # @param\n",
        "\n",
        "X_train=readh5(os.path.join(PARAMS.DS_DIR,PARAMS.X_TRAIN_IDEN))\n",
        "Y_train=readh5(os.path.join(PARAMS.DS_DIR,PARAMS.Y_TRAIN_IDEN))\n",
        "X_eval=readh5(os.path.join(PARAMS.DS_DIR,PARAMS.X_EVAL_IDEN))\n",
        "Y_eval=readh5(os.path.join(PARAMS.DS_DIR,PARAMS.Y_EVAL_IDEN))\n",
        "\n",
        "X_train=X_train.astype('float32')/255.0\n",
        "X_eval=X_eval.astype('float32')/255.0\n",
        "\n",
        "print(X_train.shape)\n",
        "print(Y_train.shape)\n",
        "print(X_eval.shape)\n",
        "print(Y_eval.shape)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(24192, 64, 64, 3)\n",
            "(24192, 2)\n",
            "(1152, 64, 64, 3)\n",
            "(1152, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blwtSzOarVYM",
        "colab_type": "text"
      },
      "source": [
        "### Compile Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOro7D1krWYf",
        "colab_type": "code",
        "outputId": "171a3b4b-76f2-4e36-a313-2768e218d5d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "from coreLib.models import DenseNet\n",
        "\n",
        "'''\n",
        "resolver = tf.contrib.cluster_resolver.TPUClusterResolver(tpu_address)\n",
        "tf.contrib.distribute.initialize_tpu_system(resolver)\n",
        "strategy = tf.contrib.distribute.TPUStrategy(resolver)\n",
        "with strategy.scope():\n",
        "  >> code here for tf>1.13.1\n",
        "'''\n",
        "MODEL_OBJ=DenseNet(\n",
        "                    image_dim=64,\n",
        "                    nb_channels=3,\n",
        "                    nb_classes=2,\n",
        "                    nb_dense=3, # @param\n",
        "                    nb_layers=12,# @param\n",
        "                    nb_filter=12,# @param \n",
        "                    growth_rate=12,# @param \n",
        "                    weight_decay=None,# @param\n",
        "                    dropout_rate=None,# @param\n",
        "                    bottleneck=False,# @param\n",
        "                    compression=0.5# @param\n",
        "                   )\n",
        "\n",
        "model=MODEL_OBJ.get_model()\n",
        "model.summary()\n",
        "model.compile(optimizer=Adam(), \n",
        "                loss=categorical_crossentropy,\n",
        "                metrics=['accuracy'])\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "MODEL_INPUT (InputLayer)        (None, 64, 64, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "INITAIAL_CONV2D_LAYER (Conv2D)  (None, 64, 64, 24)   648         MODEL_INPUT[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "BATCH_NORM_BLOCK_1_LAYER_1 (Bat (None, 64, 64, 24)   96          INITAIAL_CONV2D_LAYER[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_1_LAYER_1 (Activatio (None, 64, 64, 24)   0           BATCH_NORM_BLOCK_1_LAYER_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_1_LAYER_1 (Conv2D) (None, 64, 64, 12)   2592        RELU_BLOCK_1_LAYER_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_1_LAYER_1 (Concate (None, 64, 64, 36)   0           INITAIAL_CONV2D_LAYER[0][0]      \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "BATCH_NORM_BLOCK_1_LAYER_2 (Bat (None, 64, 64, 36)   144         CONCAT_BLOCK_1_LAYER_1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_1_LAYER_2 (Activatio (None, 64, 64, 36)   0           BATCH_NORM_BLOCK_1_LAYER_2[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_1_LAYER_2 (Conv2D) (None, 64, 64, 24)   7776        RELU_BLOCK_1_LAYER_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_1_LAYER_2 (Concate (None, 64, 64, 60)   0           INITAIAL_CONV2D_LAYER[0][0]      \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_1[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_2[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "BATCH_NORM_BLOCK_1_LAYER_3 (Bat (None, 64, 64, 60)   240         CONCAT_BLOCK_1_LAYER_2[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_1_LAYER_3 (Activatio (None, 64, 64, 60)   0           BATCH_NORM_BLOCK_1_LAYER_3[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_1_LAYER_3 (Conv2D) (None, 64, 64, 36)   19440       RELU_BLOCK_1_LAYER_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_1_LAYER_3 (Concate (None, 64, 64, 96)   0           INITAIAL_CONV2D_LAYER[0][0]      \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_1[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_2[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_3[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "BATCH_NORM_BLOCK_1_LAYER_4 (Bat (None, 64, 64, 96)   384         CONCAT_BLOCK_1_LAYER_3[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_1_LAYER_4 (Activatio (None, 64, 64, 96)   0           BATCH_NORM_BLOCK_1_LAYER_4[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_1_LAYER_4 (Conv2D) (None, 64, 64, 48)   41472       RELU_BLOCK_1_LAYER_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_1_LAYER_4 (Concate (None, 64, 64, 144)  0           INITAIAL_CONV2D_LAYER[0][0]      \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_1[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_2[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_3[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_4[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "BATCH_NORM_BLOCK_1_LAYER_5 (Bat (None, 64, 64, 144)  576         CONCAT_BLOCK_1_LAYER_4[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_1_LAYER_5 (Activatio (None, 64, 64, 144)  0           BATCH_NORM_BLOCK_1_LAYER_5[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_1_LAYER_5 (Conv2D) (None, 64, 64, 60)   77760       RELU_BLOCK_1_LAYER_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_1_LAYER_5 (Concate (None, 64, 64, 204)  0           INITAIAL_CONV2D_LAYER[0][0]      \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_1[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_2[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_3[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_4[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_5[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "BATCH_NORM_BLOCK_1_LAYER_6 (Bat (None, 64, 64, 204)  816         CONCAT_BLOCK_1_LAYER_5[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_1_LAYER_6 (Activatio (None, 64, 64, 204)  0           BATCH_NORM_BLOCK_1_LAYER_6[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_1_LAYER_6 (Conv2D) (None, 64, 64, 72)   132192      RELU_BLOCK_1_LAYER_6[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_1_LAYER_6 (Concate (None, 64, 64, 276)  0           INITAIAL_CONV2D_LAYER[0][0]      \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_1[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_2[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_3[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_4[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_5[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_6[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "BATCH_NORM_BLOCK_1_LAYER_7 (Bat (None, 64, 64, 276)  1104        CONCAT_BLOCK_1_LAYER_6[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_1_LAYER_7 (Activatio (None, 64, 64, 276)  0           BATCH_NORM_BLOCK_1_LAYER_7[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_1_LAYER_7 (Conv2D) (None, 64, 64, 84)   208656      RELU_BLOCK_1_LAYER_7[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_1_LAYER_7 (Concate (None, 64, 64, 360)  0           INITAIAL_CONV2D_LAYER[0][0]      \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_1[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_2[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_3[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_4[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_5[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_6[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_7[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "BATCH_NORM_BLOCK_1_LAYER_8 (Bat (None, 64, 64, 360)  1440        CONCAT_BLOCK_1_LAYER_7[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_1_LAYER_8 (Activatio (None, 64, 64, 360)  0           BATCH_NORM_BLOCK_1_LAYER_8[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_1_LAYER_8 (Conv2D) (None, 64, 64, 96)   311040      RELU_BLOCK_1_LAYER_8[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_1_LAYER_8 (Concate (None, 64, 64, 456)  0           INITAIAL_CONV2D_LAYER[0][0]      \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_1[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_2[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_3[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_4[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_5[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_6[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_7[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_8[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "BATCH_NORM_BLOCK_1_LAYER_9 (Bat (None, 64, 64, 456)  1824        CONCAT_BLOCK_1_LAYER_8[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_1_LAYER_9 (Activatio (None, 64, 64, 456)  0           BATCH_NORM_BLOCK_1_LAYER_9[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_1_LAYER_9 (Conv2D) (None, 64, 64, 108)  443232      RELU_BLOCK_1_LAYER_9[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_1_LAYER_9 (Concate (None, 64, 64, 564)  0           INITAIAL_CONV2D_LAYER[0][0]      \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_1[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_2[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_3[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_4[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_5[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_6[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_7[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_8[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_9[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "BATCH_NORM_BLOCK_1_LAYER_10 (Ba (None, 64, 64, 564)  2256        CONCAT_BLOCK_1_LAYER_9[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_1_LAYER_10 (Activati (None, 64, 64, 564)  0           BATCH_NORM_BLOCK_1_LAYER_10[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_1_LAYER_10 (Conv2D (None, 64, 64, 120)  609120      RELU_BLOCK_1_LAYER_10[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_1_LAYER_10 (Concat (None, 64, 64, 684)  0           INITAIAL_CONV2D_LAYER[0][0]      \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_1[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_2[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_3[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_4[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_5[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_6[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_7[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_8[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_9[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_10[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "BATCH_NORM_BLOCK_1_LAYER_11 (Ba (None, 64, 64, 684)  2736        CONCAT_BLOCK_1_LAYER_10[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_1_LAYER_11 (Activati (None, 64, 64, 684)  0           BATCH_NORM_BLOCK_1_LAYER_11[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_1_LAYER_11 (Conv2D (None, 64, 64, 132)  812592      RELU_BLOCK_1_LAYER_11[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_1_LAYER_11 (Concat (None, 64, 64, 816)  0           INITAIAL_CONV2D_LAYER[0][0]      \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_1[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_2[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_3[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_4[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_5[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_6[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_7[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_8[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_9[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_10[0][0]    \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_11[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "BATCH_NORM_BLOCK_1_LAYER_12 (Ba (None, 64, 64, 816)  3264        CONCAT_BLOCK_1_LAYER_11[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_1_LAYER_12 (Activati (None, 64, 64, 816)  0           BATCH_NORM_BLOCK_1_LAYER_12[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_1_LAYER_12 (Conv2D (None, 64, 64, 144)  1057536     RELU_BLOCK_1_LAYER_12[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_1_LAYER_12 (Concat (None, 64, 64, 960)  0           INITAIAL_CONV2D_LAYER[0][0]      \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_1[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_2[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_3[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_4[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_5[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_6[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_7[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_8[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_9[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_10[0][0]    \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_11[0][0]    \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_12[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "TRANS_BLOCK_1_BATCH_NORM (Batch (None, 64, 64, 960)  3840        CONCAT_BLOCK_1_LAYER_12[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "TRANS_BLOCK_1_RELU (Activation) (None, 64, 64, 960)  0           TRANS_BLOCK_1_BATCH_NORM[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "TRANS_BLOCK_1_CONV2D (Conv2D)   (None, 64, 64, 156)  149760      TRANS_BLOCK_1_RELU[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "TRANS_BLOCK_1_AVGPOOL (AverageP (None, 32, 32, 156)  0           TRANS_BLOCK_1_CONV2D[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "BATCH_NORM_BLOCK_2_LAYER_1 (Bat (None, 32, 32, 156)  624         TRANS_BLOCK_1_AVGPOOL[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_2_LAYER_1 (Activatio (None, 32, 32, 156)  0           BATCH_NORM_BLOCK_2_LAYER_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_2_LAYER_1 (Conv2D) (None, 32, 32, 78)   109512      RELU_BLOCK_2_LAYER_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_2_LAYER_1 (Concate (None, 32, 32, 234)  0           TRANS_BLOCK_1_AVGPOOL[0][0]      \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "BATCH_NORM_BLOCK_2_LAYER_2 (Bat (None, 32, 32, 234)  936         CONCAT_BLOCK_2_LAYER_1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_2_LAYER_2 (Activatio (None, 32, 32, 234)  0           BATCH_NORM_BLOCK_2_LAYER_2[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_2_LAYER_2 (Conv2D) (None, 32, 32, 90)   189540      RELU_BLOCK_2_LAYER_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_2_LAYER_2 (Concate (None, 32, 32, 324)  0           TRANS_BLOCK_1_AVGPOOL[0][0]      \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_1[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_2[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "BATCH_NORM_BLOCK_2_LAYER_3 (Bat (None, 32, 32, 324)  1296        CONCAT_BLOCK_2_LAYER_2[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_2_LAYER_3 (Activatio (None, 32, 32, 324)  0           BATCH_NORM_BLOCK_2_LAYER_3[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_2_LAYER_3 (Conv2D) (None, 32, 32, 102)  297432      RELU_BLOCK_2_LAYER_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_2_LAYER_3 (Concate (None, 32, 32, 426)  0           TRANS_BLOCK_1_AVGPOOL[0][0]      \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_1[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_2[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_3[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "BATCH_NORM_BLOCK_2_LAYER_4 (Bat (None, 32, 32, 426)  1704        CONCAT_BLOCK_2_LAYER_3[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_2_LAYER_4 (Activatio (None, 32, 32, 426)  0           BATCH_NORM_BLOCK_2_LAYER_4[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_2_LAYER_4 (Conv2D) (None, 32, 32, 114)  437076      RELU_BLOCK_2_LAYER_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_2_LAYER_4 (Concate (None, 32, 32, 540)  0           TRANS_BLOCK_1_AVGPOOL[0][0]      \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_1[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_2[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_3[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_4[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "BATCH_NORM_BLOCK_2_LAYER_5 (Bat (None, 32, 32, 540)  2160        CONCAT_BLOCK_2_LAYER_4[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_2_LAYER_5 (Activatio (None, 32, 32, 540)  0           BATCH_NORM_BLOCK_2_LAYER_5[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_2_LAYER_5 (Conv2D) (None, 32, 32, 126)  612360      RELU_BLOCK_2_LAYER_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_2_LAYER_5 (Concate (None, 32, 32, 666)  0           TRANS_BLOCK_1_AVGPOOL[0][0]      \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_1[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_2[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_3[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_4[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_5[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "BATCH_NORM_BLOCK_2_LAYER_6 (Bat (None, 32, 32, 666)  2664        CONCAT_BLOCK_2_LAYER_5[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_2_LAYER_6 (Activatio (None, 32, 32, 666)  0           BATCH_NORM_BLOCK_2_LAYER_6[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_2_LAYER_6 (Conv2D) (None, 32, 32, 138)  827172      RELU_BLOCK_2_LAYER_6[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_2_LAYER_6 (Concate (None, 32, 32, 804)  0           TRANS_BLOCK_1_AVGPOOL[0][0]      \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_1[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_2[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_3[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_4[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_5[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_6[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "BATCH_NORM_BLOCK_2_LAYER_7 (Bat (None, 32, 32, 804)  3216        CONCAT_BLOCK_2_LAYER_6[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_2_LAYER_7 (Activatio (None, 32, 32, 804)  0           BATCH_NORM_BLOCK_2_LAYER_7[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_2_LAYER_7 (Conv2D) (None, 32, 32, 150)  1085400     RELU_BLOCK_2_LAYER_7[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_2_LAYER_7 (Concate (None, 32, 32, 954)  0           TRANS_BLOCK_1_AVGPOOL[0][0]      \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_1[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_2[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_3[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_4[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_5[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_6[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_7[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "BATCH_NORM_BLOCK_2_LAYER_8 (Bat (None, 32, 32, 954)  3816        CONCAT_BLOCK_2_LAYER_7[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_2_LAYER_8 (Activatio (None, 32, 32, 954)  0           BATCH_NORM_BLOCK_2_LAYER_8[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_2_LAYER_8 (Conv2D) (None, 32, 32, 162)  1390932     RELU_BLOCK_2_LAYER_8[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_2_LAYER_8 (Concate (None, 32, 32, 1116) 0           TRANS_BLOCK_1_AVGPOOL[0][0]      \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_1[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_2[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_3[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_4[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_5[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_6[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_7[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_8[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "BATCH_NORM_BLOCK_2_LAYER_9 (Bat (None, 32, 32, 1116) 4464        CONCAT_BLOCK_2_LAYER_8[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_2_LAYER_9 (Activatio (None, 32, 32, 1116) 0           BATCH_NORM_BLOCK_2_LAYER_9[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_2_LAYER_9 (Conv2D) (None, 32, 32, 174)  1747656     RELU_BLOCK_2_LAYER_9[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_2_LAYER_9 (Concate (None, 32, 32, 1290) 0           TRANS_BLOCK_1_AVGPOOL[0][0]      \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_1[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_2[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_3[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_4[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_5[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_6[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_7[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_8[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_9[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "BATCH_NORM_BLOCK_2_LAYER_10 (Ba (None, 32, 32, 1290) 5160        CONCAT_BLOCK_2_LAYER_9[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_2_LAYER_10 (Activati (None, 32, 32, 1290) 0           BATCH_NORM_BLOCK_2_LAYER_10[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_2_LAYER_10 (Conv2D (None, 32, 32, 186)  2159460     RELU_BLOCK_2_LAYER_10[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_2_LAYER_10 (Concat (None, 32, 32, 1476) 0           TRANS_BLOCK_1_AVGPOOL[0][0]      \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_1[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_2[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_3[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_4[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_5[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_6[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_7[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_8[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_9[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_10[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "BATCH_NORM_BLOCK_2_LAYER_11 (Ba (None, 32, 32, 1476) 5904        CONCAT_BLOCK_2_LAYER_10[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_2_LAYER_11 (Activati (None, 32, 32, 1476) 0           BATCH_NORM_BLOCK_2_LAYER_11[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_2_LAYER_11 (Conv2D (None, 32, 32, 198)  2630232     RELU_BLOCK_2_LAYER_11[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_2_LAYER_11 (Concat (None, 32, 32, 1674) 0           TRANS_BLOCK_1_AVGPOOL[0][0]      \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_1[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_2[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_3[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_4[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_5[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_6[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_7[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_8[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_9[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_10[0][0]    \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_11[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "BATCH_NORM_BLOCK_2_LAYER_12 (Ba (None, 32, 32, 1674) 6696        CONCAT_BLOCK_2_LAYER_11[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_2_LAYER_12 (Activati (None, 32, 32, 1674) 0           BATCH_NORM_BLOCK_2_LAYER_12[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_2_LAYER_12 (Conv2D (None, 32, 32, 210)  3163860     RELU_BLOCK_2_LAYER_12[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_2_LAYER_12 (Concat (None, 32, 32, 1884) 0           TRANS_BLOCK_1_AVGPOOL[0][0]      \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_1[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_2[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_3[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_4[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_5[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_6[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_7[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_8[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_9[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_10[0][0]    \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_11[0][0]    \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_12[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "TRANS_BLOCK_2_BATCH_NORM (Batch (None, 32, 32, 1884) 7536        CONCAT_BLOCK_2_LAYER_12[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "TRANS_BLOCK_2_RELU (Activation) (None, 32, 32, 1884) 0           TRANS_BLOCK_2_BATCH_NORM[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "TRANS_BLOCK_2_CONV2D (Conv2D)   (None, 32, 32, 222)  418248      TRANS_BLOCK_2_RELU[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "TRANS_BLOCK_2_AVGPOOL (AverageP (None, 16, 16, 222)  0           TRANS_BLOCK_2_CONV2D[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "BATCH_NORM_BLOCK_3_LAYER_1 (Bat (None, 16, 16, 222)  888         TRANS_BLOCK_2_AVGPOOL[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_3_LAYER_1 (Activatio (None, 16, 16, 222)  0           BATCH_NORM_BLOCK_3_LAYER_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_3_LAYER_1 (Conv2D) (None, 16, 16, 111)  221778      RELU_BLOCK_3_LAYER_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_3_LAYER_1 (Concate (None, 16, 16, 333)  0           TRANS_BLOCK_2_AVGPOOL[0][0]      \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "BATCH_NORM_BLOCK_3_LAYER_2 (Bat (None, 16, 16, 333)  1332        CONCAT_BLOCK_3_LAYER_1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_3_LAYER_2 (Activatio (None, 16, 16, 333)  0           BATCH_NORM_BLOCK_3_LAYER_2[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_3_LAYER_2 (Conv2D) (None, 16, 16, 123)  368631      RELU_BLOCK_3_LAYER_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_3_LAYER_2 (Concate (None, 16, 16, 456)  0           TRANS_BLOCK_2_AVGPOOL[0][0]      \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_1[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_2[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "BATCH_NORM_BLOCK_3_LAYER_3 (Bat (None, 16, 16, 456)  1824        CONCAT_BLOCK_3_LAYER_2[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_3_LAYER_3 (Activatio (None, 16, 16, 456)  0           BATCH_NORM_BLOCK_3_LAYER_3[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_3_LAYER_3 (Conv2D) (None, 16, 16, 135)  554040      RELU_BLOCK_3_LAYER_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_3_LAYER_3 (Concate (None, 16, 16, 591)  0           TRANS_BLOCK_2_AVGPOOL[0][0]      \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_1[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_2[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_3[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "BATCH_NORM_BLOCK_3_LAYER_4 (Bat (None, 16, 16, 591)  2364        CONCAT_BLOCK_3_LAYER_3[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_3_LAYER_4 (Activatio (None, 16, 16, 591)  0           BATCH_NORM_BLOCK_3_LAYER_4[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_3_LAYER_4 (Conv2D) (None, 16, 16, 147)  781893      RELU_BLOCK_3_LAYER_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_3_LAYER_4 (Concate (None, 16, 16, 738)  0           TRANS_BLOCK_2_AVGPOOL[0][0]      \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_1[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_2[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_3[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_4[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "BATCH_NORM_BLOCK_3_LAYER_5 (Bat (None, 16, 16, 738)  2952        CONCAT_BLOCK_3_LAYER_4[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_3_LAYER_5 (Activatio (None, 16, 16, 738)  0           BATCH_NORM_BLOCK_3_LAYER_5[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_3_LAYER_5 (Conv2D) (None, 16, 16, 159)  1056078     RELU_BLOCK_3_LAYER_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_3_LAYER_5 (Concate (None, 16, 16, 897)  0           TRANS_BLOCK_2_AVGPOOL[0][0]      \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_1[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_2[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_3[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_4[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_5[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "BATCH_NORM_BLOCK_3_LAYER_6 (Bat (None, 16, 16, 897)  3588        CONCAT_BLOCK_3_LAYER_5[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_3_LAYER_6 (Activatio (None, 16, 16, 897)  0           BATCH_NORM_BLOCK_3_LAYER_6[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_3_LAYER_6 (Conv2D) (None, 16, 16, 171)  1380483     RELU_BLOCK_3_LAYER_6[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_3_LAYER_6 (Concate (None, 16, 16, 1068) 0           TRANS_BLOCK_2_AVGPOOL[0][0]      \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_1[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_2[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_3[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_4[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_5[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_6[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "BATCH_NORM_BLOCK_3_LAYER_7 (Bat (None, 16, 16, 1068) 4272        CONCAT_BLOCK_3_LAYER_6[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_3_LAYER_7 (Activatio (None, 16, 16, 1068) 0           BATCH_NORM_BLOCK_3_LAYER_7[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_3_LAYER_7 (Conv2D) (None, 16, 16, 183)  1758996     RELU_BLOCK_3_LAYER_7[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_3_LAYER_7 (Concate (None, 16, 16, 1251) 0           TRANS_BLOCK_2_AVGPOOL[0][0]      \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_1[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_2[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_3[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_4[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_5[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_6[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_7[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "BATCH_NORM_BLOCK_3_LAYER_8 (Bat (None, 16, 16, 1251) 5004        CONCAT_BLOCK_3_LAYER_7[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_3_LAYER_8 (Activatio (None, 16, 16, 1251) 0           BATCH_NORM_BLOCK_3_LAYER_8[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_3_LAYER_8 (Conv2D) (None, 16, 16, 195)  2195505     RELU_BLOCK_3_LAYER_8[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_3_LAYER_8 (Concate (None, 16, 16, 1446) 0           TRANS_BLOCK_2_AVGPOOL[0][0]      \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_1[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_2[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_3[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_4[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_5[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_6[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_7[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_8[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "BATCH_NORM_BLOCK_3_LAYER_9 (Bat (None, 16, 16, 1446) 5784        CONCAT_BLOCK_3_LAYER_8[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_3_LAYER_9 (Activatio (None, 16, 16, 1446) 0           BATCH_NORM_BLOCK_3_LAYER_9[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_3_LAYER_9 (Conv2D) (None, 16, 16, 207)  2693898     RELU_BLOCK_3_LAYER_9[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_3_LAYER_9 (Concate (None, 16, 16, 1653) 0           TRANS_BLOCK_2_AVGPOOL[0][0]      \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_1[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_2[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_3[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_4[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_5[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_6[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_7[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_8[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_9[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "BATCH_NORM_BLOCK_3_LAYER_10 (Ba (None, 16, 16, 1653) 6612        CONCAT_BLOCK_3_LAYER_9[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_3_LAYER_10 (Activati (None, 16, 16, 1653) 0           BATCH_NORM_BLOCK_3_LAYER_10[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_3_LAYER_10 (Conv2D (None, 16, 16, 219)  3258063     RELU_BLOCK_3_LAYER_10[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_3_LAYER_10 (Concat (None, 16, 16, 1872) 0           TRANS_BLOCK_2_AVGPOOL[0][0]      \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_1[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_2[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_3[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_4[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_5[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_6[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_7[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_8[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_9[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_10[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "BATCH_NORM_BLOCK_3_LAYER_11 (Ba (None, 16, 16, 1872) 7488        CONCAT_BLOCK_3_LAYER_10[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_3_LAYER_11 (Activati (None, 16, 16, 1872) 0           BATCH_NORM_BLOCK_3_LAYER_11[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_3_LAYER_11 (Conv2D (None, 16, 16, 231)  3891888     RELU_BLOCK_3_LAYER_11[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_3_LAYER_11 (Concat (None, 16, 16, 2103) 0           TRANS_BLOCK_2_AVGPOOL[0][0]      \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_1[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_2[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_3[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_4[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_5[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_6[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_7[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_8[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_9[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_10[0][0]    \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_11[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "BATCH_NORM_BLOCK_3_LAYER_12 (Ba (None, 16, 16, 2103) 8412        CONCAT_BLOCK_3_LAYER_11[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_3_LAYER_12 (Activati (None, 16, 16, 2103) 0           BATCH_NORM_BLOCK_3_LAYER_12[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_3_LAYER_12 (Conv2D (None, 16, 16, 243)  4599261     RELU_BLOCK_3_LAYER_12[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_3_LAYER_12 (Concat (None, 16, 16, 2346) 0           TRANS_BLOCK_2_AVGPOOL[0][0]      \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_1[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_2[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_3[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_4[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_5[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_6[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_7[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_8[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_9[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_10[0][0]    \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_11[0][0]    \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_12[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "BATCH_NORM_FINAL (BatchNormaliz (None, 16, 16, 2346) 9384        CONCAT_BLOCK_3_LAYER_12[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "FINAL_RELU (Activation)         (None, 16, 16, 2346) 0           BATCH_NORM_FINAL[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "GLOBAL_POOL (GlobalAveragePooli (None, 2346)         0           FINAL_RELU[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "CLASS_DENSE (Dense)             (None, 2)            4694        GLOBAL_POOL[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 41,832,704\n",
            "Trainable params: 41,770,304\n",
            "Non-trainable params: 62,400\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nQXx8YRNSyr",
        "colab_type": "text"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWTUQlweNYYN",
        "colab_type": "code",
        "outputId": "776fcd9d-28d3-4cbf-b6a1-ed78c83d728c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#from tensorflow.keras.callbacks import ModelCheckpoint   \n",
        "#checkpointer = ModelCheckpoint(filepath=os.path.join(PARAMS.MODEL_DIR,'{}.h5'.format(PARAMS.MODEL_NAME)), \n",
        "                               #verbose=1, \n",
        "                               #save_best_only=True)\n",
        "TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "model = tf.contrib.tpu.keras_to_tpu_model(model,\n",
        "                                          strategy=tf.contrib.tpu.TPUDistributionStrategy(tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))\n",
        "history=model.fit(X_train,Y_train,\n",
        "                  validation_data=(X_eval,Y_eval),\n",
        "                  epochs=PARAMS.NUM_EPOCHS,\n",
        "                  batch_size=PARAMS.BATCH_SIZE, \n",
        "                  verbose=1)\n",
        "#callbacks=[checkpointer],"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.54.23.18:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 13047164762070634554)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 6597454410549484217)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 8340505189229715248)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 12683387582170140513)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 13441517617839392952)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 12283731854165827101)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 14209654070803323468)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 6893700225016749847)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 10446599126649232301)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 4348892694973092535)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 13796428094455521304)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.0010000000474974513, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.0010000000474974513, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "Train on 24192 samples, validate on 1152 samples\n",
            "Epoch 1/100\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(16,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(16, 64, 64, 3), dtype=tf.float32, name='MODEL_INPUT_10'), TensorSpec(shape=(16, 2), dtype=tf.float32, name='CLASS_DENSE_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.0010000000474974513, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "INFO:tensorflow:Remapping placeholder for MODEL_INPUT\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py:302: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.Adam object at 0x7f25449be390> []\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 78.41433262825012 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            "INFO:tensorflow:CPU -> TPU lr: 0.0010000000474974513 {0.001}\n",
            "INFO:tensorflow:CPU -> TPU beta_1: 0.8999999761581421 {0.9}\n",
            "INFO:tensorflow:CPU -> TPU beta_2: 0.9990000128746033 {0.999}\n",
            "INFO:tensorflow:CPU -> TPU decay: 0.0 {0.0}\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "24064/24192 [============================>.] - ETA: 1s - loss: 0.6808 - acc: 0.6156INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(16,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(16, 64, 64, 3), dtype=tf.float32, name='MODEL_INPUT_10'), TensorSpec(shape=(16, 2), dtype=tf.float32, name='CLASS_DENSE_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.0010000000474974513, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "INFO:tensorflow:Remapping placeholder for MODEL_INPUT\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.Adam object at 0x7f252ca28ef0> []\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 30.116639852523804 secs\n",
            "24192/24192 [==============================] - 285s 12ms/sample - loss: 0.6806 - acc: 0.6157 - val_loss: 0.7203 - val_acc: 0.5191\n",
            "Epoch 2/100\n",
            "24192/24192 [==============================] - 81s 3ms/sample - loss: 0.6040 - acc: 0.6656 - val_loss: 0.6794 - val_acc: 0.5816\n",
            "Epoch 3/100\n",
            "24192/24192 [==============================] - 81s 3ms/sample - loss: 0.5797 - acc: 0.6934 - val_loss: 0.7779 - val_acc: 0.5720\n",
            "Epoch 4/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.5595 - acc: 0.7062 - val_loss: 1.5751 - val_acc: 0.6484\n",
            "Epoch 5/100\n",
            "24192/24192 [==============================] - 81s 3ms/sample - loss: 0.5370 - acc: 0.7263 - val_loss: 3.4210 - val_acc: 0.5495\n",
            "Epoch 6/100\n",
            "24192/24192 [==============================] - 81s 3ms/sample - loss: 0.5101 - acc: 0.7476 - val_loss: 0.4877 - val_acc: 0.7474\n",
            "Epoch 7/100\n",
            "24192/24192 [==============================] - 81s 3ms/sample - loss: 0.4849 - acc: 0.7665 - val_loss: 1.2431 - val_acc: 0.6571\n",
            "Epoch 8/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.4705 - acc: 0.7718 - val_loss: 0.8212 - val_acc: 0.6250\n",
            "Epoch 9/100\n",
            "24192/24192 [==============================] - 81s 3ms/sample - loss: 0.4534 - acc: 0.7830 - val_loss: 0.5969 - val_acc: 0.7326\n",
            "Epoch 10/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.4207 - acc: 0.8029 - val_loss: 0.5109 - val_acc: 0.7587\n",
            "Epoch 11/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.3961 - acc: 0.8152 - val_loss: 0.4911 - val_acc: 0.7674\n",
            "Epoch 12/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.3927 - acc: 0.8221 - val_loss: 0.8938 - val_acc: 0.7023\n",
            "Epoch 13/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.3792 - acc: 0.8273 - val_loss: 0.4790 - val_acc: 0.7734\n",
            "Epoch 14/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.3398 - acc: 0.8486 - val_loss: 0.4503 - val_acc: 0.8047\n",
            "Epoch 15/100\n",
            "24192/24192 [==============================] - 81s 3ms/sample - loss: 0.3291 - acc: 0.8548 - val_loss: 0.5094 - val_acc: 0.7700\n",
            "Epoch 16/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.3031 - acc: 0.8696 - val_loss: 0.5794 - val_acc: 0.7352\n",
            "Epoch 17/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.2772 - acc: 0.8824 - val_loss: 0.3044 - val_acc: 0.8819\n",
            "Epoch 18/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.2613 - acc: 0.8895 - val_loss: 0.2792 - val_acc: 0.8785\n",
            "Epoch 19/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.2441 - acc: 0.8981 - val_loss: 0.2339 - val_acc: 0.9071\n",
            "Epoch 20/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.2242 - acc: 0.9088 - val_loss: 0.3753 - val_acc: 0.8394\n",
            "Epoch 21/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.2133 - acc: 0.9134 - val_loss: 0.3950 - val_acc: 0.8316\n",
            "Epoch 22/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.2070 - acc: 0.9172 - val_loss: 0.6799 - val_acc: 0.8394\n",
            "Epoch 23/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.1820 - acc: 0.9276 - val_loss: 0.4568 - val_acc: 0.8194\n",
            "Epoch 24/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.1704 - acc: 0.9327 - val_loss: 0.4037 - val_acc: 0.8307\n",
            "Epoch 25/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.1592 - acc: 0.9386 - val_loss: 0.5048 - val_acc: 0.8307\n",
            "Epoch 26/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.1413 - acc: 0.9456 - val_loss: 0.1702 - val_acc: 0.9392\n",
            "Epoch 27/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.1424 - acc: 0.9462 - val_loss: 0.1200 - val_acc: 0.9549\n",
            "Epoch 28/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.1365 - acc: 0.9475 - val_loss: 2.7630 - val_acc: 0.6155\n",
            "Epoch 29/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.1430 - acc: 0.9446 - val_loss: 0.1378 - val_acc: 0.9505\n",
            "Epoch 30/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.1161 - acc: 0.9556 - val_loss: 0.2165 - val_acc: 0.9280\n",
            "Epoch 31/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.1158 - acc: 0.9554 - val_loss: 1.1029 - val_acc: 0.7344\n",
            "Epoch 32/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.1003 - acc: 0.9616 - val_loss: 0.2722 - val_acc: 0.8967\n",
            "Epoch 33/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.0989 - acc: 0.9627 - val_loss: 0.2281 - val_acc: 0.9427\n",
            "Epoch 34/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.0884 - acc: 0.9688 - val_loss: 0.2752 - val_acc: 0.9141\n",
            "Epoch 35/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.0963 - acc: 0.9627 - val_loss: 0.9495 - val_acc: 0.8203\n",
            "Epoch 36/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.0826 - acc: 0.9702 - val_loss: 0.2877 - val_acc: 0.8958\n",
            "Epoch 37/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.0812 - acc: 0.9698 - val_loss: 0.2755 - val_acc: 0.9115\n",
            "Epoch 38/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.0739 - acc: 0.9732 - val_loss: 0.5360 - val_acc: 0.8394\n",
            "Epoch 39/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.0615 - acc: 0.9788 - val_loss: 0.1461 - val_acc: 0.9488\n",
            "Epoch 40/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.0749 - acc: 0.9716 - val_loss: 0.1416 - val_acc: 0.9453\n",
            "Epoch 41/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.0590 - acc: 0.9788 - val_loss: 0.1813 - val_acc: 0.9253\n",
            "Epoch 42/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.0601 - acc: 0.9789 - val_loss: 1.6256 - val_acc: 0.7257\n",
            "Epoch 43/100\n",
            "24192/24192 [==============================] - 79s 3ms/sample - loss: 0.0628 - acc: 0.9781 - val_loss: 1.2137 - val_acc: 0.7700\n",
            "Epoch 44/100\n",
            "24192/24192 [==============================] - 79s 3ms/sample - loss: 0.0588 - acc: 0.9791 - val_loss: 0.8489 - val_acc: 0.8203\n",
            "Epoch 45/100\n",
            "24192/24192 [==============================] - 79s 3ms/sample - loss: 0.0660 - acc: 0.9761 - val_loss: 0.2426 - val_acc: 0.9184\n",
            "Epoch 46/100\n",
            "24192/24192 [==============================] - 79s 3ms/sample - loss: 0.0528 - acc: 0.9815 - val_loss: 0.1525 - val_acc: 0.9410\n",
            "Epoch 47/100\n",
            "24192/24192 [==============================] - 79s 3ms/sample - loss: 0.0492 - acc: 0.9831 - val_loss: 0.1182 - val_acc: 0.9661\n",
            "Epoch 48/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.0462 - acc: 0.9831 - val_loss: 0.1318 - val_acc: 0.9549\n",
            "Epoch 49/100\n",
            "24192/24192 [==============================] - 79s 3ms/sample - loss: 0.0506 - acc: 0.9810 - val_loss: 1.5797 - val_acc: 0.7465\n",
            "Epoch 50/100\n",
            "24192/24192 [==============================] - 79s 3ms/sample - loss: 0.0419 - acc: 0.9854 - val_loss: 0.2092 - val_acc: 0.9366\n",
            "Epoch 51/100\n",
            "24192/24192 [==============================] - 79s 3ms/sample - loss: 0.0379 - acc: 0.9862 - val_loss: 0.0920 - val_acc: 0.9679\n",
            "Epoch 52/100\n",
            "24192/24192 [==============================] - 79s 3ms/sample - loss: 0.0504 - acc: 0.9816 - val_loss: 0.3251 - val_acc: 0.9175\n",
            "Epoch 53/100\n",
            "24192/24192 [==============================] - 79s 3ms/sample - loss: 0.0357 - acc: 0.9881 - val_loss: 0.0979 - val_acc: 0.9722\n",
            "Epoch 54/100\n",
            "24192/24192 [==============================] - 79s 3ms/sample - loss: 0.0388 - acc: 0.9864 - val_loss: 0.2367 - val_acc: 0.9297\n",
            "Epoch 55/100\n",
            "24192/24192 [==============================] - 79s 3ms/sample - loss: 0.0371 - acc: 0.9871 - val_loss: 0.1880 - val_acc: 0.9366\n",
            "Epoch 56/100\n",
            "24192/24192 [==============================] - 79s 3ms/sample - loss: 0.0531 - acc: 0.9806 - val_loss: 1.4914 - val_acc: 0.7821\n",
            "Epoch 57/100\n",
            "24192/24192 [==============================] - 79s 3ms/sample - loss: 0.0286 - acc: 0.9903 - val_loss: 0.2636 - val_acc: 0.9132\n",
            "Epoch 58/100\n",
            "24192/24192 [==============================] - 79s 3ms/sample - loss: 0.0481 - acc: 0.9827 - val_loss: 0.0567 - val_acc: 0.9844\n",
            "Epoch 59/100\n",
            "24192/24192 [==============================] - 79s 3ms/sample - loss: 0.0353 - acc: 0.9871 - val_loss: 1.7848 - val_acc: 0.7483\n",
            "Epoch 60/100\n",
            "24192/24192 [==============================] - 79s 3ms/sample - loss: 0.0370 - acc: 0.9872 - val_loss: 0.2360 - val_acc: 0.9366\n",
            "Epoch 61/100\n",
            "24192/24192 [==============================] - 79s 3ms/sample - loss: 0.0369 - acc: 0.9880 - val_loss: 0.0986 - val_acc: 0.9635\n",
            "Epoch 62/100\n",
            "24192/24192 [==============================] - 79s 3ms/sample - loss: 0.0261 - acc: 0.9909 - val_loss: 0.2346 - val_acc: 0.9245\n",
            "Epoch 63/100\n",
            "24192/24192 [==============================] - 79s 3ms/sample - loss: 0.0341 - acc: 0.9876 - val_loss: 0.5710 - val_acc: 0.8898\n",
            "Epoch 64/100\n",
            "24192/24192 [==============================] - 79s 3ms/sample - loss: 0.0276 - acc: 0.9896 - val_loss: 0.0269 - val_acc: 0.9878\n",
            "Epoch 65/100\n",
            "24192/24192 [==============================] - 79s 3ms/sample - loss: 0.0299 - acc: 0.9897 - val_loss: 0.3035 - val_acc: 0.9253\n",
            "Epoch 66/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.0317 - acc: 0.9892 - val_loss: 0.1958 - val_acc: 0.9401\n",
            "Epoch 67/100\n",
            "24192/24192 [==============================] - 79s 3ms/sample - loss: 0.0249 - acc: 0.9914 - val_loss: 0.0753 - val_acc: 0.9757\n",
            "Epoch 68/100\n",
            "24192/24192 [==============================] - 79s 3ms/sample - loss: 0.0308 - acc: 0.9894 - val_loss: 0.1068 - val_acc: 0.9714\n",
            "Epoch 69/100\n",
            "24192/24192 [==============================] - 79s 3ms/sample - loss: 0.0343 - acc: 0.9878 - val_loss: 1.7138 - val_acc: 0.7587\n",
            "Epoch 70/100\n",
            "24192/24192 [==============================] - 79s 3ms/sample - loss: 0.0239 - acc: 0.9920 - val_loss: 0.0700 - val_acc: 0.9774\n",
            "Epoch 71/100\n",
            "24192/24192 [==============================] - 79s 3ms/sample - loss: 0.0360 - acc: 0.9871 - val_loss: 0.0189 - val_acc: 0.9896\n",
            "Epoch 72/100\n",
            "24192/24192 [==============================] - 79s 3ms/sample - loss: 0.0157 - acc: 0.9949 - val_loss: 0.0377 - val_acc: 0.9870\n",
            "Epoch 73/100\n",
            "24192/24192 [==============================] - 79s 3ms/sample - loss: 0.0242 - acc: 0.9919 - val_loss: 0.2555 - val_acc: 0.9366\n",
            "Epoch 74/100\n",
            "24192/24192 [==============================] - 79s 3ms/sample - loss: 0.0344 - acc: 0.9876 - val_loss: 0.0522 - val_acc: 0.9844\n",
            "Epoch 75/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.0154 - acc: 0.9944 - val_loss: 0.0200 - val_acc: 0.9887\n",
            "Epoch 76/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.0326 - acc: 0.9892 - val_loss: 0.5075 - val_acc: 0.8924\n",
            "Epoch 77/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.0248 - acc: 0.9902 - val_loss: 0.1835 - val_acc: 0.9349\n",
            "Epoch 78/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.0180 - acc: 0.9940 - val_loss: 0.0315 - val_acc: 0.9870\n",
            "Epoch 79/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.0292 - acc: 0.9898 - val_loss: 0.4944 - val_acc: 0.8672\n",
            "Epoch 80/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.0247 - acc: 0.9911 - val_loss: 0.0831 - val_acc: 0.9653\n",
            "Epoch 81/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.0271 - acc: 0.9908 - val_loss: 0.0435 - val_acc: 0.9844\n",
            "Epoch 82/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.0150 - acc: 0.9947 - val_loss: 0.2406 - val_acc: 0.9288\n",
            "Epoch 83/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.0198 - acc: 0.9928 - val_loss: 0.0866 - val_acc: 0.9731\n",
            "Epoch 84/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.0239 - acc: 0.9917 - val_loss: 1.1318 - val_acc: 0.8108\n",
            "Epoch 85/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.0255 - acc: 0.9912 - val_loss: 0.1963 - val_acc: 0.9470\n",
            "Epoch 86/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.0184 - acc: 0.9933 - val_loss: 0.0655 - val_acc: 0.9748\n",
            "Epoch 87/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.0221 - acc: 0.9933 - val_loss: 0.3918 - val_acc: 0.8837\n",
            "Epoch 88/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.0124 - acc: 0.9961 - val_loss: 0.1420 - val_acc: 0.9644\n",
            "Epoch 89/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.0207 - acc: 0.9930 - val_loss: 0.0467 - val_acc: 0.9861\n",
            "Epoch 90/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.0153 - acc: 0.9956 - val_loss: 0.0391 - val_acc: 0.9887\n",
            "Epoch 91/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.0250 - acc: 0.9918 - val_loss: 0.0082 - val_acc: 0.9965\n",
            "Epoch 92/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.0088 - acc: 0.9970 - val_loss: 0.0261 - val_acc: 0.9896\n",
            "Epoch 93/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.0266 - acc: 0.9907 - val_loss: 0.0963 - val_acc: 0.9661\n",
            "Epoch 94/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.0174 - acc: 0.9940 - val_loss: 0.0610 - val_acc: 0.9835\n",
            "Epoch 95/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.0262 - acc: 0.9906 - val_loss: 0.0484 - val_acc: 0.9826\n",
            "Epoch 96/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.0099 - acc: 0.9967 - val_loss: 0.2346 - val_acc: 0.9384\n",
            "Epoch 97/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.0059 - acc: 0.9983 - val_loss: 0.7834 - val_acc: 0.8802\n",
            "Epoch 98/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.0330 - acc: 0.9893 - val_loss: 0.2876 - val_acc: 0.9332\n",
            "Epoch 99/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.0155 - acc: 0.9946 - val_loss: 0.1371 - val_acc: 0.9635\n",
            "Epoch 100/100\n",
            "24192/24192 [==============================] - 80s 3ms/sample - loss: 0.0170 - acc: 0.9947 - val_loss: 0.0212 - val_acc: 0.9913\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzx4-kyYbGfI",
        "colab_type": "text"
      },
      "source": [
        "### F1 SCORE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1iTz86KbNed",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "e65a9943-8c2c-450c-bdaf-f850a6dd17fb"
      },
      "source": [
        "from sklearn import metrics\n",
        "X_TEST_H5_DIR='/content/gdrive/My Drive/PROJECTS/MED/Test/X_test.h5' # @param\n",
        "Y_TEST_H5_DIR='/content/gdrive/My Drive/PROJECTS/MED/Test/Y_test.h5' # @param\n",
        "x_test=readh5(X_TEST_H5_DIR)\n",
        "y_test=readh5(Y_TEST_H5_DIR)\n",
        "x_test=x_test.astype('float32')/255.0\n",
        "y_true =[np.argmax(y) for y in y_test]\n",
        "total_data=x_test.shape[0]\n",
        "y_pred=[]\n",
        "for i in range(0,total_data,PARAMS.BATCH_SIZE):\n",
        "  y_batch=[]\n",
        "  x_batch=x_test[i:i+PARAMS.BATCH_SIZE]\n",
        "  y_batch=model.predict(x_batch)\n",
        "  for y_data in y_batch:\n",
        "    y_pred.append(np.argmax(y_data))\n",
        "f1_accuracy =100* metrics.f1_score(y_true,y_pred, average = 'micro')\n",
        "print('F1 SCORE:{} % '.format(f1_accuracy))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=infer (# of cores 8), [TensorSpec(shape=(4, 64, 64, 3), dtype=tf.float32, name='MODEL_INPUT_10')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.0010000000474974513, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "INFO:tensorflow:Remapping placeholder for MODEL_INPUT\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 27.817744731903076 secs\n",
            "F1 SCORE:99.1390306122449 % \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1M9sjHIbSnf",
        "colab_type": "text"
      },
      "source": [
        "### Plot Training Histoty"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndNSS7XIbXaK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "outputId": "0ae3c17a-a63c-4f48-a646-1232e93e4f46"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('LOSS History')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.savefig(os.path.join(PARAMS.MODEL_DIR,'{}_history.png'.format(PARAMS.MODEL_NAME)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTzTkMY4hQ2i",
        "colab_type": "text"
      },
      "source": [
        "### Save Model Weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iTmgJkYhQQz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "9bee22a4-e543-4c25-fc90-f4c6f70afb4c"
      },
      "source": [
        "model.save_weights(os.path.join(PARAMS.MODEL_DIR,'{}_final.h5'.format(PARAMS.MODEL_NAME)))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Copying TPU weights to the CPU\n",
            "INFO:tensorflow:TPU -> CPU lr: 0.0010000000474974513\n",
            "INFO:tensorflow:TPU -> CPU beta_1: 0.8999999761581421\n",
            "INFO:tensorflow:TPU -> CPU beta_2: 0.9990000128746033\n",
            "INFO:tensorflow:TPU -> CPU decay: 0.0\n",
            "INFO:tensorflow:TPU -> CPU epsilon: 1e-07\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "INFO:tensorflow:TPU -> CPU amsgrad: False\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}