{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_denseNet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mnansary/pyF2O/blob/master/colab_gen_unet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ojVYZ7Spzpv",
        "colab_type": "text"
      },
      "source": [
        "# colab specific task\n",
        "*   mount google drive\n",
        "*   change working directory to git repo\n",
        "*   TPU check\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOI03Qu0tHrf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "outputId": "9593c3b3-2d96-4ff6-fb48-dbdf50baef82"
      },
      "source": [
        "!pip3 install tensorflow==1.13.1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==1.13.1 in /usr/local/lib/python3.6/dist-packages (1.13.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.15.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (3.10.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.2.2)\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.13.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.17.4)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.33.6)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.0.8)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (0.8.1)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.1) (1.13.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.1) (41.6.0)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow==1.13.1) (3.0.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (2.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (0.16.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--q4JaV2ps6z",
        "colab_type": "code",
        "outputId": "7c094f72-59cb-4fa6-b933-1f8871df7f7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsVhQoAOqGGW",
        "colab_type": "code",
        "outputId": "16152928-4805-4eda-abc5-13f7dc782073",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/gdrive/My\\ Drive/PROJECTS/MED/pyCONVCLASSIFIER"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/PROJECTS/MED/pyCONVCLASSIFIER\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "692-uM-yqdaF",
        "colab_type": "text"
      },
      "source": [
        "## TPU Check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JT9QkSiJqed6",
        "colab_type": "code",
        "outputId": "52d6e6e7-9cfd-4b0f-e10f-a66e27797387",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        }
      },
      "source": [
        "import os\n",
        "import pprint\n",
        "import tensorflow as tf\n",
        "\n",
        "if 'COLAB_TPU_ADDR' not in os.environ:\n",
        "  print('ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!')\n",
        "else:\n",
        "  tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "  print ('TPU address is', tpu_address)\n",
        "\n",
        "  with tf.Session(tpu_address) as session:\n",
        "    devices = session.list_devices()\n",
        "    \n",
        "  print('TPU devices:')\n",
        "  pprint.pprint(devices)\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TPU address is grpc://10.109.145.162:8470\n",
            "TPU devices:\n",
            "[_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 465241026486723486),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 14933552930580039901),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 16966322663973662171),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 7900512790020774135),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 9492904540330817090),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 9172989839193564669),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 9936916729616966597),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 1720852002758136561),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 5751872623888881328),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 18009093840847593879),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 6102599166480015289)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.13.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxefiHZ4qlHA",
        "colab_type": "text"
      },
      "source": [
        "# DenseNet Model Training\n",
        "* Define **Parameters for training**\n",
        "* Define **FLAGS for DenseNet**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2usCWY1Fc6E",
        "colab_type": "text"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tdzez_PiFVTM",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "38c31d15-d5a9-4725-d784-044cf8955e77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "from coreLib.utils import readh5\n",
        "import numpy as np \n",
        "class PARAMS:\n",
        "    DS_DIR  = '/content/gdrive/My Drive/PROJECTS/MED/Train/' # @param\n",
        "    BATCH_SIZE      = 1024  # @param\n",
        "    NUM_EPOCHS      = 250  # @param\n",
        "    X_TRAIN_IDEN    = 'X_train.h5'  # @param\n",
        "    Y_TRAIN_IDEN    = 'Y_train.h5'  # @param\n",
        "    X_EVAL_IDEN     = 'X_eval.h5'  # @param\n",
        "    Y_EVAL_IDEN     = 'Y_eval.h5'  # @param\n",
        "    MODEL_DIR       = '/content/gdrive/My Drive/PROJECTS/MED/MODEL_DIR/' # @param\n",
        "    MODEL_NAME      = 'denseNet' # @param\n",
        "\n",
        "X_train=readh5(os.path.join(PARAMS.DS_DIR,PARAMS.X_TRAIN_IDEN))\n",
        "Y_train=readh5(os.path.join(PARAMS.DS_DIR,PARAMS.Y_TRAIN_IDEN))\n",
        "X_eval=readh5(os.path.join(PARAMS.DS_DIR,PARAMS.X_EVAL_IDEN))\n",
        "Y_eval=readh5(os.path.join(PARAMS.DS_DIR,PARAMS.Y_EVAL_IDEN))\n",
        "\n",
        "X_train=X_train.astype('float32')/255.0\n",
        "X_eval=X_eval.astype('float32')/255.0\n",
        "\n",
        "print(X_train.shape)\n",
        "print(Y_train.shape)\n",
        "print(X_eval.shape)\n",
        "print(Y_eval.shape)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(26624, 64, 64, 1)\n",
            "(26624, 2)\n",
            "(1024, 64, 64, 1)\n",
            "(1024, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blwtSzOarVYM",
        "colab_type": "text"
      },
      "source": [
        "### Compile Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOro7D1krWYf",
        "colab_type": "code",
        "outputId": "7faa056d-96b5-452e-aada-2de037edb857",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "from coreLib.models import DenseNet\n",
        "\n",
        "'''\n",
        "resolver = tf.contrib.cluster_resolver.TPUClusterResolver(tpu_address)\n",
        "tf.contrib.distribute.initialize_tpu_system(resolver)\n",
        "strategy = tf.contrib.distribute.TPUStrategy(resolver)\n",
        "with strategy.scope():\n",
        "  >> code here for tf>1.13.1\n",
        "'''\n",
        "MODEL_OBJ=DenseNet(\n",
        "                    image_dim=64,     # @param\n",
        "                    nb_channels=1,    # @param\n",
        "                    nb_classes=2,     # @param\n",
        "                    nb_dense=4,       # @param\n",
        "                    nb_layers=12,     # @param\n",
        "                    nb_filter=16,     # @param\n",
        "                    growth_rate=12,   # @param\n",
        "                    weight_decay=1E-4,# @param\n",
        "                    dropout_rate=None # @param\n",
        "                   )\n",
        "\n",
        "model=MODEL_OBJ.get_model()\n",
        "model.summary()\n",
        "model.compile(optimizer=Adam(), \n",
        "                loss=categorical_crossentropy,\n",
        "                metrics=['accuracy'])\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "MODEL_INPUT (InputLayer)        (None, 64, 64, 1)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "INITAIAL_CONV2D_LAYER (Conv2D)  (None, 64, 64, 32)   288         MODEL_INPUT[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "INITIAL_BATCH_NORM_LAYER (Batch (None, 64, 64, 32)   128         INITAIAL_CONV2D_LAYER[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_1_LAYER_1 (Activatio (None, 64, 64, 32)   0           INITIAL_BATCH_NORM_LAYER[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_1_LAYER_1 (Conv2D) (None, 64, 64, 16)   4608        RELU_BLOCK_1_LAYER_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_1_LAYER_2 (Activatio (None, 64, 64, 32)   0           INITIAL_BATCH_NORM_LAYER[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_1_LAYER_1 (Concate (None, 64, 64, 48)   0           INITIAL_BATCH_NORM_LAYER[0][0]   \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_1_LAYER_2 (Conv2D) (None, 64, 64, 28)   8064        RELU_BLOCK_1_LAYER_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_1_LAYER_3 (Activatio (None, 64, 64, 32)   0           INITIAL_BATCH_NORM_LAYER[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_1_LAYER_2 (Concate (None, 64, 64, 76)   0           CONCAT_BLOCK_1_LAYER_1[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_2[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_1_LAYER_3 (Conv2D) (None, 64, 64, 40)   11520       RELU_BLOCK_1_LAYER_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_1_LAYER_4 (Activatio (None, 64, 64, 32)   0           INITIAL_BATCH_NORM_LAYER[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_1_LAYER_3 (Concate (None, 64, 64, 116)  0           CONCAT_BLOCK_1_LAYER_2[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_3[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_1_LAYER_4 (Conv2D) (None, 64, 64, 52)   14976       RELU_BLOCK_1_LAYER_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_1_LAYER_5 (Activatio (None, 64, 64, 32)   0           INITIAL_BATCH_NORM_LAYER[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_1_LAYER_4 (Concate (None, 64, 64, 168)  0           CONCAT_BLOCK_1_LAYER_3[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_4[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_1_LAYER_5 (Conv2D) (None, 64, 64, 64)   18432       RELU_BLOCK_1_LAYER_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_1_LAYER_6 (Activatio (None, 64, 64, 32)   0           INITIAL_BATCH_NORM_LAYER[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_1_LAYER_5 (Concate (None, 64, 64, 232)  0           CONCAT_BLOCK_1_LAYER_4[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_5[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_1_LAYER_6 (Conv2D) (None, 64, 64, 76)   21888       RELU_BLOCK_1_LAYER_6[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_1_LAYER_7 (Activatio (None, 64, 64, 32)   0           INITIAL_BATCH_NORM_LAYER[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_1_LAYER_6 (Concate (None, 64, 64, 308)  0           CONCAT_BLOCK_1_LAYER_5[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_6[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_1_LAYER_7 (Conv2D) (None, 64, 64, 88)   25344       RELU_BLOCK_1_LAYER_7[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_1_LAYER_8 (Activatio (None, 64, 64, 32)   0           INITIAL_BATCH_NORM_LAYER[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_1_LAYER_7 (Concate (None, 64, 64, 396)  0           CONCAT_BLOCK_1_LAYER_6[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_7[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_1_LAYER_8 (Conv2D) (None, 64, 64, 100)  28800       RELU_BLOCK_1_LAYER_8[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_1_LAYER_9 (Activatio (None, 64, 64, 32)   0           INITIAL_BATCH_NORM_LAYER[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_1_LAYER_8 (Concate (None, 64, 64, 496)  0           CONCAT_BLOCK_1_LAYER_7[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_8[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_1_LAYER_9 (Conv2D) (None, 64, 64, 112)  32256       RELU_BLOCK_1_LAYER_9[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_1_LAYER_10 (Activati (None, 64, 64, 32)   0           INITIAL_BATCH_NORM_LAYER[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_1_LAYER_9 (Concate (None, 64, 64, 608)  0           CONCAT_BLOCK_1_LAYER_8[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_9[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_1_LAYER_10 (Conv2D (None, 64, 64, 124)  35712       RELU_BLOCK_1_LAYER_10[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_1_LAYER_11 (Activati (None, 64, 64, 32)   0           INITIAL_BATCH_NORM_LAYER[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_1_LAYER_10 (Concat (None, 64, 64, 732)  0           CONCAT_BLOCK_1_LAYER_9[0][0]     \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_10[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_1_LAYER_11 (Conv2D (None, 64, 64, 136)  39168       RELU_BLOCK_1_LAYER_11[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_1_LAYER_12 (Activati (None, 64, 64, 32)   0           INITIAL_BATCH_NORM_LAYER[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_1_LAYER_11 (Concat (None, 64, 64, 868)  0           CONCAT_BLOCK_1_LAYER_10[0][0]    \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_11[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_1_LAYER_12 (Conv2D (None, 64, 64, 148)  42624       RELU_BLOCK_1_LAYER_12[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_1_LAYER_12 (Concat (None, 64, 64, 1016) 0           CONCAT_BLOCK_1_LAYER_11[0][0]    \n",
            "                                                                 CONV2D_BLOCK_1_LAYER_12[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "TRANS_BLOCK_1_CONV2D (Conv2D)   (None, 64, 64, 160)  162560      CONCAT_BLOCK_1_LAYER_12[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "TRANS_BLOCK_1_AVGPOOL (AverageP (None, 32, 32, 160)  0           TRANS_BLOCK_1_CONV2D[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "TRANS_BLOCK_1_BATCH_NORM (Batch (None, 32, 32, 160)  640         TRANS_BLOCK_1_AVGPOOL[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_2_LAYER_1 (Activatio (None, 32, 32, 160)  0           TRANS_BLOCK_1_BATCH_NORM[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_2_LAYER_1 (Conv2D) (None, 32, 32, 160)  230400      RELU_BLOCK_2_LAYER_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_2_LAYER_2 (Activatio (None, 32, 32, 160)  0           TRANS_BLOCK_1_BATCH_NORM[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_2_LAYER_1 (Concate (None, 32, 32, 320)  0           TRANS_BLOCK_1_BATCH_NORM[0][0]   \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_2_LAYER_2 (Conv2D) (None, 32, 32, 172)  247680      RELU_BLOCK_2_LAYER_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_2_LAYER_3 (Activatio (None, 32, 32, 160)  0           TRANS_BLOCK_1_BATCH_NORM[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_2_LAYER_2 (Concate (None, 32, 32, 492)  0           CONCAT_BLOCK_2_LAYER_1[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_2[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_2_LAYER_3 (Conv2D) (None, 32, 32, 184)  264960      RELU_BLOCK_2_LAYER_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_2_LAYER_4 (Activatio (None, 32, 32, 160)  0           TRANS_BLOCK_1_BATCH_NORM[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_2_LAYER_3 (Concate (None, 32, 32, 676)  0           CONCAT_BLOCK_2_LAYER_2[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_3[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_2_LAYER_4 (Conv2D) (None, 32, 32, 196)  282240      RELU_BLOCK_2_LAYER_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_2_LAYER_5 (Activatio (None, 32, 32, 160)  0           TRANS_BLOCK_1_BATCH_NORM[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_2_LAYER_4 (Concate (None, 32, 32, 872)  0           CONCAT_BLOCK_2_LAYER_3[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_4[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_2_LAYER_5 (Conv2D) (None, 32, 32, 208)  299520      RELU_BLOCK_2_LAYER_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_2_LAYER_6 (Activatio (None, 32, 32, 160)  0           TRANS_BLOCK_1_BATCH_NORM[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_2_LAYER_5 (Concate (None, 32, 32, 1080) 0           CONCAT_BLOCK_2_LAYER_4[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_5[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_2_LAYER_6 (Conv2D) (None, 32, 32, 220)  316800      RELU_BLOCK_2_LAYER_6[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_2_LAYER_7 (Activatio (None, 32, 32, 160)  0           TRANS_BLOCK_1_BATCH_NORM[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_2_LAYER_6 (Concate (None, 32, 32, 1300) 0           CONCAT_BLOCK_2_LAYER_5[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_6[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_2_LAYER_7 (Conv2D) (None, 32, 32, 232)  334080      RELU_BLOCK_2_LAYER_7[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_2_LAYER_8 (Activatio (None, 32, 32, 160)  0           TRANS_BLOCK_1_BATCH_NORM[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_2_LAYER_7 (Concate (None, 32, 32, 1532) 0           CONCAT_BLOCK_2_LAYER_6[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_7[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_2_LAYER_8 (Conv2D) (None, 32, 32, 244)  351360      RELU_BLOCK_2_LAYER_8[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_2_LAYER_9 (Activatio (None, 32, 32, 160)  0           TRANS_BLOCK_1_BATCH_NORM[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_2_LAYER_8 (Concate (None, 32, 32, 1776) 0           CONCAT_BLOCK_2_LAYER_7[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_8[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_2_LAYER_9 (Conv2D) (None, 32, 32, 256)  368640      RELU_BLOCK_2_LAYER_9[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_2_LAYER_10 (Activati (None, 32, 32, 160)  0           TRANS_BLOCK_1_BATCH_NORM[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_2_LAYER_9 (Concate (None, 32, 32, 2032) 0           CONCAT_BLOCK_2_LAYER_8[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_9[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_2_LAYER_10 (Conv2D (None, 32, 32, 268)  385920      RELU_BLOCK_2_LAYER_10[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_2_LAYER_11 (Activati (None, 32, 32, 160)  0           TRANS_BLOCK_1_BATCH_NORM[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_2_LAYER_10 (Concat (None, 32, 32, 2300) 0           CONCAT_BLOCK_2_LAYER_9[0][0]     \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_10[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_2_LAYER_11 (Conv2D (None, 32, 32, 280)  403200      RELU_BLOCK_2_LAYER_11[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_2_LAYER_12 (Activati (None, 32, 32, 160)  0           TRANS_BLOCK_1_BATCH_NORM[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_2_LAYER_11 (Concat (None, 32, 32, 2580) 0           CONCAT_BLOCK_2_LAYER_10[0][0]    \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_11[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_2_LAYER_12 (Conv2D (None, 32, 32, 292)  420480      RELU_BLOCK_2_LAYER_12[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_2_LAYER_12 (Concat (None, 32, 32, 2872) 0           CONCAT_BLOCK_2_LAYER_11[0][0]    \n",
            "                                                                 CONV2D_BLOCK_2_LAYER_12[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "TRANS_BLOCK_2_CONV2D (Conv2D)   (None, 32, 32, 304)  873088      CONCAT_BLOCK_2_LAYER_12[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "TRANS_BLOCK_2_AVGPOOL (AverageP (None, 16, 16, 304)  0           TRANS_BLOCK_2_CONV2D[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "TRANS_BLOCK_2_BATCH_NORM (Batch (None, 16, 16, 304)  1216        TRANS_BLOCK_2_AVGPOOL[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_3_LAYER_1 (Activatio (None, 16, 16, 304)  0           TRANS_BLOCK_2_BATCH_NORM[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_3_LAYER_1 (Conv2D) (None, 16, 16, 304)  831744      RELU_BLOCK_3_LAYER_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_3_LAYER_2 (Activatio (None, 16, 16, 304)  0           TRANS_BLOCK_2_BATCH_NORM[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_3_LAYER_1 (Concate (None, 16, 16, 608)  0           TRANS_BLOCK_2_BATCH_NORM[0][0]   \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_3_LAYER_2 (Conv2D) (None, 16, 16, 316)  864576      RELU_BLOCK_3_LAYER_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_3_LAYER_3 (Activatio (None, 16, 16, 304)  0           TRANS_BLOCK_2_BATCH_NORM[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_3_LAYER_2 (Concate (None, 16, 16, 924)  0           CONCAT_BLOCK_3_LAYER_1[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_2[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_3_LAYER_3 (Conv2D) (None, 16, 16, 328)  897408      RELU_BLOCK_3_LAYER_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_3_LAYER_4 (Activatio (None, 16, 16, 304)  0           TRANS_BLOCK_2_BATCH_NORM[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_3_LAYER_3 (Concate (None, 16, 16, 1252) 0           CONCAT_BLOCK_3_LAYER_2[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_3[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_3_LAYER_4 (Conv2D) (None, 16, 16, 340)  930240      RELU_BLOCK_3_LAYER_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_3_LAYER_5 (Activatio (None, 16, 16, 304)  0           TRANS_BLOCK_2_BATCH_NORM[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_3_LAYER_4 (Concate (None, 16, 16, 1592) 0           CONCAT_BLOCK_3_LAYER_3[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_4[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_3_LAYER_5 (Conv2D) (None, 16, 16, 352)  963072      RELU_BLOCK_3_LAYER_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_3_LAYER_6 (Activatio (None, 16, 16, 304)  0           TRANS_BLOCK_2_BATCH_NORM[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_3_LAYER_5 (Concate (None, 16, 16, 1944) 0           CONCAT_BLOCK_3_LAYER_4[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_5[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_3_LAYER_6 (Conv2D) (None, 16, 16, 364)  995904      RELU_BLOCK_3_LAYER_6[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_3_LAYER_7 (Activatio (None, 16, 16, 304)  0           TRANS_BLOCK_2_BATCH_NORM[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_3_LAYER_6 (Concate (None, 16, 16, 2308) 0           CONCAT_BLOCK_3_LAYER_5[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_6[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_3_LAYER_7 (Conv2D) (None, 16, 16, 376)  1028736     RELU_BLOCK_3_LAYER_7[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_3_LAYER_8 (Activatio (None, 16, 16, 304)  0           TRANS_BLOCK_2_BATCH_NORM[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_3_LAYER_7 (Concate (None, 16, 16, 2684) 0           CONCAT_BLOCK_3_LAYER_6[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_7[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_3_LAYER_8 (Conv2D) (None, 16, 16, 388)  1061568     RELU_BLOCK_3_LAYER_8[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_3_LAYER_9 (Activatio (None, 16, 16, 304)  0           TRANS_BLOCK_2_BATCH_NORM[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_3_LAYER_8 (Concate (None, 16, 16, 3072) 0           CONCAT_BLOCK_3_LAYER_7[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_8[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_3_LAYER_9 (Conv2D) (None, 16, 16, 400)  1094400     RELU_BLOCK_3_LAYER_9[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_3_LAYER_10 (Activati (None, 16, 16, 304)  0           TRANS_BLOCK_2_BATCH_NORM[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_3_LAYER_9 (Concate (None, 16, 16, 3472) 0           CONCAT_BLOCK_3_LAYER_8[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_9[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_3_LAYER_10 (Conv2D (None, 16, 16, 412)  1127232     RELU_BLOCK_3_LAYER_10[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_3_LAYER_11 (Activati (None, 16, 16, 304)  0           TRANS_BLOCK_2_BATCH_NORM[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_3_LAYER_10 (Concat (None, 16, 16, 3884) 0           CONCAT_BLOCK_3_LAYER_9[0][0]     \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_10[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_3_LAYER_11 (Conv2D (None, 16, 16, 424)  1160064     RELU_BLOCK_3_LAYER_11[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_3_LAYER_12 (Activati (None, 16, 16, 304)  0           TRANS_BLOCK_2_BATCH_NORM[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_3_LAYER_11 (Concat (None, 16, 16, 4308) 0           CONCAT_BLOCK_3_LAYER_10[0][0]    \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_11[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_3_LAYER_12 (Conv2D (None, 16, 16, 436)  1192896     RELU_BLOCK_3_LAYER_12[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_3_LAYER_12 (Concat (None, 16, 16, 4744) 0           CONCAT_BLOCK_3_LAYER_11[0][0]    \n",
            "                                                                 CONV2D_BLOCK_3_LAYER_12[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "TRANS_BLOCK_3_CONV2D (Conv2D)   (None, 16, 16, 448)  2125312     CONCAT_BLOCK_3_LAYER_12[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "TRANS_BLOCK_3_AVGPOOL (AverageP (None, 8, 8, 448)    0           TRANS_BLOCK_3_CONV2D[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "TRANS_BLOCK_3_BATCH_NORM (Batch (None, 8, 8, 448)    1792        TRANS_BLOCK_3_AVGPOOL[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_4_LAYER_1 (Activatio (None, 8, 8, 448)    0           TRANS_BLOCK_3_BATCH_NORM[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_4_LAYER_1 (Conv2D) (None, 8, 8, 448)    1806336     RELU_BLOCK_4_LAYER_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_4_LAYER_2 (Activatio (None, 8, 8, 448)    0           TRANS_BLOCK_3_BATCH_NORM[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_4_LAYER_1 (Concate (None, 8, 8, 896)    0           TRANS_BLOCK_3_BATCH_NORM[0][0]   \n",
            "                                                                 CONV2D_BLOCK_4_LAYER_1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_4_LAYER_2 (Conv2D) (None, 8, 8, 460)    1854720     RELU_BLOCK_4_LAYER_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_4_LAYER_3 (Activatio (None, 8, 8, 448)    0           TRANS_BLOCK_3_BATCH_NORM[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_4_LAYER_2 (Concate (None, 8, 8, 1356)   0           CONCAT_BLOCK_4_LAYER_1[0][0]     \n",
            "                                                                 CONV2D_BLOCK_4_LAYER_2[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_4_LAYER_3 (Conv2D) (None, 8, 8, 472)    1903104     RELU_BLOCK_4_LAYER_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_4_LAYER_4 (Activatio (None, 8, 8, 448)    0           TRANS_BLOCK_3_BATCH_NORM[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_4_LAYER_3 (Concate (None, 8, 8, 1828)   0           CONCAT_BLOCK_4_LAYER_2[0][0]     \n",
            "                                                                 CONV2D_BLOCK_4_LAYER_3[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_4_LAYER_4 (Conv2D) (None, 8, 8, 484)    1951488     RELU_BLOCK_4_LAYER_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_4_LAYER_5 (Activatio (None, 8, 8, 448)    0           TRANS_BLOCK_3_BATCH_NORM[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_4_LAYER_4 (Concate (None, 8, 8, 2312)   0           CONCAT_BLOCK_4_LAYER_3[0][0]     \n",
            "                                                                 CONV2D_BLOCK_4_LAYER_4[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_4_LAYER_5 (Conv2D) (None, 8, 8, 496)    1999872     RELU_BLOCK_4_LAYER_5[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_4_LAYER_6 (Activatio (None, 8, 8, 448)    0           TRANS_BLOCK_3_BATCH_NORM[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_4_LAYER_5 (Concate (None, 8, 8, 2808)   0           CONCAT_BLOCK_4_LAYER_4[0][0]     \n",
            "                                                                 CONV2D_BLOCK_4_LAYER_5[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_4_LAYER_6 (Conv2D) (None, 8, 8, 508)    2048256     RELU_BLOCK_4_LAYER_6[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_4_LAYER_7 (Activatio (None, 8, 8, 448)    0           TRANS_BLOCK_3_BATCH_NORM[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_4_LAYER_6 (Concate (None, 8, 8, 3316)   0           CONCAT_BLOCK_4_LAYER_5[0][0]     \n",
            "                                                                 CONV2D_BLOCK_4_LAYER_6[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_4_LAYER_7 (Conv2D) (None, 8, 8, 520)    2096640     RELU_BLOCK_4_LAYER_7[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_4_LAYER_8 (Activatio (None, 8, 8, 448)    0           TRANS_BLOCK_3_BATCH_NORM[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_4_LAYER_7 (Concate (None, 8, 8, 3836)   0           CONCAT_BLOCK_4_LAYER_6[0][0]     \n",
            "                                                                 CONV2D_BLOCK_4_LAYER_7[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_4_LAYER_8 (Conv2D) (None, 8, 8, 532)    2145024     RELU_BLOCK_4_LAYER_8[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_4_LAYER_9 (Activatio (None, 8, 8, 448)    0           TRANS_BLOCK_3_BATCH_NORM[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_4_LAYER_8 (Concate (None, 8, 8, 4368)   0           CONCAT_BLOCK_4_LAYER_7[0][0]     \n",
            "                                                                 CONV2D_BLOCK_4_LAYER_8[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_4_LAYER_9 (Conv2D) (None, 8, 8, 544)    2193408     RELU_BLOCK_4_LAYER_9[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_4_LAYER_10 (Activati (None, 8, 8, 448)    0           TRANS_BLOCK_3_BATCH_NORM[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_4_LAYER_9 (Concate (None, 8, 8, 4912)   0           CONCAT_BLOCK_4_LAYER_8[0][0]     \n",
            "                                                                 CONV2D_BLOCK_4_LAYER_9[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_4_LAYER_10 (Conv2D (None, 8, 8, 556)    2241792     RELU_BLOCK_4_LAYER_10[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_4_LAYER_11 (Activati (None, 8, 8, 448)    0           TRANS_BLOCK_3_BATCH_NORM[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_4_LAYER_10 (Concat (None, 8, 8, 5468)   0           CONCAT_BLOCK_4_LAYER_9[0][0]     \n",
            "                                                                 CONV2D_BLOCK_4_LAYER_10[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_4_LAYER_11 (Conv2D (None, 8, 8, 568)    2290176     RELU_BLOCK_4_LAYER_11[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "RELU_BLOCK_4_LAYER_12 (Activati (None, 8, 8, 448)    0           TRANS_BLOCK_3_BATCH_NORM[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_4_LAYER_11 (Concat (None, 8, 8, 6036)   0           CONCAT_BLOCK_4_LAYER_10[0][0]    \n",
            "                                                                 CONV2D_BLOCK_4_LAYER_11[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "CONV2D_BLOCK_4_LAYER_12 (Conv2D (None, 8, 8, 580)    2338560     RELU_BLOCK_4_LAYER_12[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_BLOCK_4_LAYER_12 (Concat (None, 8, 8, 6616)   0           CONCAT_BLOCK_4_LAYER_11[0][0]    \n",
            "                                                                 CONV2D_BLOCK_4_LAYER_12[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "FINAL_RELU (Activation)         (None, 8, 8, 6616)   0           CONCAT_BLOCK_4_LAYER_12[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "GLOBAL_POOL (GlobalAveragePooli (None, 6616)         0           FINAL_RELU[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "CLASS_DENSE (Dense)             (None, 2)            13234       GLOBAL_POOL[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 44,384,146\n",
            "Trainable params: 44,382,258\n",
            "Non-trainable params: 1,888\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nQXx8YRNSyr",
        "colab_type": "text"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWTUQlweNYYN",
        "colab_type": "code",
        "outputId": "a7d156aa-a4c7-44a6-9129-70b05bdd9076",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#from tensorflow.keras.callbacks import ModelCheckpoint   \n",
        "#checkpointer = ModelCheckpoint(filepath=os.path.join(PARAMS.MODEL_DIR,'{}.h5'.format(PARAMS.MODEL_NAME)), \n",
        "                               #verbose=1, \n",
        "                               #save_best_only=True)\n",
        "TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "model = tf.contrib.tpu.keras_to_tpu_model(model,strategy=tf.contrib.tpu.TPUDistributionStrategy(tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))\n",
        "history=model.fit(X_train,Y_train,\n",
        "                  validation_data=(X_eval,Y_eval),\n",
        "                  epochs=PARAMS.NUM_EPOCHS,\n",
        "                  batch_size=PARAMS.BATCH_SIZE, \n",
        "                  verbose=1)\n",
        "#callbacks=[checkpointer],"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "INFO:tensorflow:Querying Tensorflow master (grpc://10.109.145.162:8470) for TPU system metadata.\n",
            "INFO:tensorflow:Found TPU system:\n",
            "INFO:tensorflow:*** Num TPU Cores: 8\n",
            "INFO:tensorflow:*** Num TPU Workers: 1\n",
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 465241026486723486)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 14933552930580039901)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 16966322663973662171)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 7900512790020774135)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 9492904540330817090)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 9172989839193564669)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 9936916729616966597)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 1720852002758136561)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 5751872623888881328)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 18009093840847593879)\n",
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 6102599166480015289)\n",
            "WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.0010000000474974513, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.0010000000474974513, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "Train on 26624 samples, validate on 1024 samples\n",
            "Epoch 1/250\n",
            "INFO:tensorflow:New input shapes; (re-)compiling: mode=train (# of cores 8), [TensorSpec(shape=(128,), dtype=tf.int32, name='core_id0'), TensorSpec(shape=(128, 64, 64, 1), dtype=tf.float32, name='MODEL_INPUT_10'), TensorSpec(shape=(128, 2), dtype=tf.float32, name='CLASS_DENSE_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.0010000000474974513, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "INFO:tensorflow:Remapping placeholder for MODEL_INPUT\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/tpu/python/tpu/keras_support.py:302: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.Adam object at 0x7f8f9061eda0> []\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 65.53151750564575 secs\n",
            "INFO:tensorflow:Setting weights on TPU model.\n",
            "INFO:tensorflow:CPU -> TPU lr: 0.0010000000474974513 {0.001}\n",
            "INFO:tensorflow:CPU -> TPU beta_1: 0.8999999761581421 {0.9}\n",
            "INFO:tensorflow:CPU -> TPU beta_2: 0.9990000128746033 {0.999}\n",
            "INFO:tensorflow:CPU -> TPU decay: 0.0 {0.0}\n",
            "WARNING:tensorflow:Cannot update non-variable config: epsilon\n",
            "WARNING:tensorflow:Cannot update non-variable config: amsgrad\n",
            "25600/26624 [===========================>..] - ETA: 5s - loss: 9.2441 - acc: 0.4954 INFO:tensorflow:New input shapes; (re-)compiling: mode=eval (# of cores 8), [TensorSpec(shape=(128,), dtype=tf.int32, name='core_id_10'), TensorSpec(shape=(128, 64, 64, 1), dtype=tf.float32, name='MODEL_INPUT_10'), TensorSpec(shape=(128, 2), dtype=tf.float32, name='CLASS_DENSE_target_30')]\n",
            "INFO:tensorflow:Overriding default placeholder.\n",
            "INFO:tensorflow:Cloning Adam {'lr': 0.0010000000474974513, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'decay': 0.0, 'epsilon': 1e-07, 'amsgrad': False}\n",
            "INFO:tensorflow:Remapping placeholder for MODEL_INPUT\n",
            "INFO:tensorflow:KerasCrossShard: <tensorflow.python.keras.optimizers.Adam object at 0x7f8f83696390> []\n",
            "INFO:tensorflow:Started compiling\n",
            "INFO:tensorflow:Finished compiling. Time elapsed: 18.989002466201782 secs\n",
            "26624/26624 [==============================] - 160s 6ms/sample - loss: 9.2493 - acc: 0.4954 - val_loss: 9.0970 - val_acc: 0.5117\n",
            "Epoch 2/250\n",
            "26624/26624 [==============================] - 20s 757us/sample - loss: 9.1848 - acc: 0.4959 - val_loss: 8.7696 - val_acc: 0.5117\n",
            "Epoch 3/250\n",
            "26624/26624 [==============================] - 20s 751us/sample - loss: 8.9075 - acc: 0.4959 - val_loss: 8.5424 - val_acc: 0.5117\n",
            "Epoch 4/250\n",
            "26624/26624 [==============================] - 20s 757us/sample - loss: 8.7163 - acc: 0.4959 - val_loss: 8.3849 - val_acc: 0.5117\n",
            "Epoch 5/250\n",
            "26624/26624 [==============================] - 20s 759us/sample - loss: 8.5822 - acc: 0.4959 - val_loss: 8.2727 - val_acc: 0.5117\n",
            "Epoch 6/250\n",
            "26624/26624 [==============================] - 20s 752us/sample - loss: 8.4856 - acc: 0.4959 - val_loss: 8.1909 - val_acc: 0.5117\n",
            "Epoch 7/250\n",
            "26624/26624 [==============================] - 20s 753us/sample - loss: 8.4145 - acc: 0.4959 - val_loss: 8.1300 - val_acc: 0.5117\n",
            "Epoch 8/250\n",
            "26624/26624 [==============================] - 20s 755us/sample - loss: 8.3611 - acc: 0.4959 - val_loss: 8.0839 - val_acc: 0.5117\n",
            "Epoch 9/250\n",
            "26624/26624 [==============================] - 20s 755us/sample - loss: 8.3204 - acc: 0.4959 - val_loss: 8.0484 - val_acc: 0.5117\n",
            "Epoch 10/250\n",
            "26624/26624 [==============================] - 20s 754us/sample - loss: 8.2889 - acc: 0.4959 - val_loss: 8.0208 - val_acc: 0.5117\n",
            "Epoch 11/250\n",
            "26624/26624 [==============================] - 20s 751us/sample - loss: 8.2641 - acc: 0.4959 - val_loss: 7.9989 - val_acc: 0.5117\n",
            "Epoch 12/250\n",
            "26624/26624 [==============================] - 20s 749us/sample - loss: 8.2444 - acc: 0.4959 - val_loss: 7.9814 - val_acc: 0.5117\n",
            "Epoch 13/250\n",
            "26624/26624 [==============================] - 20s 758us/sample - loss: 8.2285 - acc: 0.4959 - val_loss: 7.9671 - val_acc: 0.5117\n",
            "Epoch 14/250\n",
            "26624/26624 [==============================] - 20s 757us/sample - loss: 8.2156 - acc: 0.4959 - val_loss: 7.9555 - val_acc: 0.5117\n",
            "Epoch 15/250\n",
            "26624/26624 [==============================] - 20s 753us/sample - loss: 8.2050 - acc: 0.4959 - val_loss: 7.9458 - val_acc: 0.5117\n",
            "Epoch 16/250\n",
            "26624/26624 [==============================] - 20s 752us/sample - loss: 6.6984 - acc: 0.5013 - val_loss: 8.4794 - val_acc: 0.4883\n",
            "Epoch 17/250\n",
            "26624/26624 [==============================] - 20s 754us/sample - loss: 1.2054 - acc: 0.5641 - val_loss: 8.6368 - val_acc: 0.4883\n",
            "Epoch 18/250\n",
            "26624/26624 [==============================] - 20s 753us/sample - loss: 1.2060 - acc: 0.6010 - val_loss: 8.7436 - val_acc: 0.4883\n",
            "Epoch 19/250\n",
            "26624/26624 [==============================] - 20s 753us/sample - loss: 1.0954 - acc: 0.6100 - val_loss: 7.1782 - val_acc: 0.4883\n",
            "Epoch 20/250\n",
            "26624/26624 [==============================] - 20s 751us/sample - loss: 0.9974 - acc: 0.6208 - val_loss: 3.0383 - val_acc: 0.4883\n",
            "Epoch 21/250\n",
            "26624/26624 [==============================] - 20s 751us/sample - loss: 0.9269 - acc: 0.6316 - val_loss: 1.3074 - val_acc: 0.4951\n",
            "Epoch 22/250\n",
            "26624/26624 [==============================] - 20s 750us/sample - loss: 0.8767 - acc: 0.6382 - val_loss: 0.8857 - val_acc: 0.5869\n",
            "Epoch 23/250\n",
            "26624/26624 [==============================] - 20s 753us/sample - loss: 0.8403 - acc: 0.6355 - val_loss: 0.9167 - val_acc: 0.5361\n",
            "Epoch 24/250\n",
            "26624/26624 [==============================] - 20s 751us/sample - loss: 0.8087 - acc: 0.6398 - val_loss: 0.9804 - val_acc: 0.5117\n",
            "Epoch 25/250\n",
            "26624/26624 [==============================] - 20s 752us/sample - loss: 0.7846 - acc: 0.6491 - val_loss: 1.0098 - val_acc: 0.5117\n",
            "Epoch 26/250\n",
            "26624/26624 [==============================] - 20s 752us/sample - loss: 0.7592 - acc: 0.6619 - val_loss: 1.0286 - val_acc: 0.5117\n",
            "Epoch 27/250\n",
            "26624/26624 [==============================] - 20s 752us/sample - loss: 0.7431 - acc: 0.6672 - val_loss: 1.0195 - val_acc: 0.5117\n",
            "Epoch 28/250\n",
            "26624/26624 [==============================] - 20s 749us/sample - loss: 0.7290 - acc: 0.6716 - val_loss: 0.9610 - val_acc: 0.5117\n",
            "Epoch 29/250\n",
            "26624/26624 [==============================] - 20s 752us/sample - loss: 0.7152 - acc: 0.6741 - val_loss: 1.0060 - val_acc: 0.5117\n",
            "Epoch 30/250\n",
            "26624/26624 [==============================] - 20s 752us/sample - loss: 0.7074 - acc: 0.6751 - val_loss: 0.9427 - val_acc: 0.5117\n",
            "Epoch 31/250\n",
            "26624/26624 [==============================] - 20s 750us/sample - loss: 0.6969 - acc: 0.6818 - val_loss: 0.8973 - val_acc: 0.5117\n",
            "Epoch 32/250\n",
            "26624/26624 [==============================] - 20s 749us/sample - loss: 0.6873 - acc: 0.6853 - val_loss: 0.9010 - val_acc: 0.5117\n",
            "Epoch 33/250\n",
            "26624/26624 [==============================] - 20s 752us/sample - loss: 0.6782 - acc: 0.6908 - val_loss: 0.9052 - val_acc: 0.5117\n",
            "Epoch 34/250\n",
            "26624/26624 [==============================] - 20s 751us/sample - loss: 0.6690 - acc: 0.6966 - val_loss: 0.8645 - val_acc: 0.5146\n",
            "Epoch 35/250\n",
            "26624/26624 [==============================] - 20s 752us/sample - loss: 0.6609 - acc: 0.6952 - val_loss: 0.8955 - val_acc: 0.5117\n",
            "Epoch 36/250\n",
            "26624/26624 [==============================] - 20s 756us/sample - loss: 0.6542 - acc: 0.6990 - val_loss: 0.7966 - val_acc: 0.5449\n",
            "Epoch 37/250\n",
            "26624/26624 [==============================] - 20s 755us/sample - loss: 0.6457 - acc: 0.7059 - val_loss: 0.8096 - val_acc: 0.5283\n",
            "Epoch 38/250\n",
            "26624/26624 [==============================] - 20s 757us/sample - loss: 0.6431 - acc: 0.7073 - val_loss: 0.8001 - val_acc: 0.5225\n",
            "Epoch 39/250\n",
            "26624/26624 [==============================] - 20s 756us/sample - loss: 0.6380 - acc: 0.7079 - val_loss: 0.7805 - val_acc: 0.5342\n",
            "Epoch 40/250\n",
            "26624/26624 [==============================] - 20s 750us/sample - loss: 0.6443 - acc: 0.7014 - val_loss: 0.7287 - val_acc: 0.6006\n",
            "Epoch 41/250\n",
            "26624/26624 [==============================] - 20s 752us/sample - loss: 0.6281 - acc: 0.7158 - val_loss: 0.7568 - val_acc: 0.5615\n",
            "Epoch 42/250\n",
            "26624/26624 [==============================] - 20s 752us/sample - loss: 0.6158 - acc: 0.7140 - val_loss: 0.7462 - val_acc: 0.5693\n",
            "Epoch 43/250\n",
            "26624/26624 [==============================] - 20s 750us/sample - loss: 0.6165 - acc: 0.7192 - val_loss: 0.7276 - val_acc: 0.5986\n",
            "Epoch 44/250\n",
            "26624/26624 [==============================] - 20s 750us/sample - loss: 0.6126 - acc: 0.7176 - val_loss: 0.7162 - val_acc: 0.6221\n",
            "Epoch 45/250\n",
            "26624/26624 [==============================] - 20s 757us/sample - loss: 0.6042 - acc: 0.7231 - val_loss: 0.7298 - val_acc: 0.5986\n",
            "Epoch 46/250\n",
            "26624/26624 [==============================] - 20s 756us/sample - loss: 0.6100 - acc: 0.7202 - val_loss: 0.7194 - val_acc: 0.6045\n",
            "Epoch 47/250\n",
            "26624/26624 [==============================] - 20s 755us/sample - loss: 0.5992 - acc: 0.7234 - val_loss: 0.7241 - val_acc: 0.6211\n",
            "Epoch 48/250\n",
            "26624/26624 [==============================] - 20s 757us/sample - loss: 0.5964 - acc: 0.7304 - val_loss: 0.6998 - val_acc: 0.6406\n",
            "Epoch 49/250\n",
            "26624/26624 [==============================] - 20s 747us/sample - loss: 0.5904 - acc: 0.7343 - val_loss: 0.7444 - val_acc: 0.6240\n",
            "Epoch 50/250\n",
            "26624/26624 [==============================] - 20s 749us/sample - loss: 0.5891 - acc: 0.7364 - val_loss: 0.9751 - val_acc: 0.5107\n",
            "Epoch 51/250\n",
            "26624/26624 [==============================] - 20s 745us/sample - loss: 0.5891 - acc: 0.7339 - val_loss: 0.6951 - val_acc: 0.6592\n",
            "Epoch 52/250\n",
            "26624/26624 [==============================] - 20s 747us/sample - loss: 0.5774 - acc: 0.7447 - val_loss: 0.7792 - val_acc: 0.5547\n",
            "Epoch 53/250\n",
            "26624/26624 [==============================] - 20s 748us/sample - loss: 0.5761 - acc: 0.7492 - val_loss: 0.6940 - val_acc: 0.6660\n",
            "Epoch 54/250\n",
            "26624/26624 [==============================] - 20s 751us/sample - loss: 0.5722 - acc: 0.7492 - val_loss: 0.7556 - val_acc: 0.6240\n",
            "Epoch 55/250\n",
            "26624/26624 [==============================] - 20s 749us/sample - loss: 0.5812 - acc: 0.7428 - val_loss: 0.8459 - val_acc: 0.5674\n",
            "Epoch 56/250\n",
            "26624/26624 [==============================] - 20s 750us/sample - loss: 0.5618 - acc: 0.7594 - val_loss: 0.9099 - val_acc: 0.5713\n",
            "Epoch 57/250\n",
            "26624/26624 [==============================] - 20s 747us/sample - loss: 0.5491 - acc: 0.7675 - val_loss: 0.7161 - val_acc: 0.6562\n",
            "Epoch 58/250\n",
            "26624/26624 [==============================] - 20s 748us/sample - loss: 0.5392 - acc: 0.7696 - val_loss: 0.7572 - val_acc: 0.6494\n",
            "Epoch 59/250\n",
            "26624/26624 [==============================] - 20s 748us/sample - loss: 0.5363 - acc: 0.7776 - val_loss: 0.7539 - val_acc: 0.6240\n",
            "Epoch 60/250\n",
            "26624/26624 [==============================] - 20s 746us/sample - loss: 0.5207 - acc: 0.7845 - val_loss: 0.8619 - val_acc: 0.5918\n",
            "Epoch 61/250\n",
            "26624/26624 [==============================] - 20s 745us/sample - loss: 0.5368 - acc: 0.7731 - val_loss: 0.6393 - val_acc: 0.7207\n",
            "Epoch 62/250\n",
            "26624/26624 [==============================] - 20s 746us/sample - loss: 0.5265 - acc: 0.7837 - val_loss: 0.7438 - val_acc: 0.6504\n",
            "Epoch 63/250\n",
            "26624/26624 [==============================] - 20s 752us/sample - loss: 0.5154 - acc: 0.7909 - val_loss: 0.8063 - val_acc: 0.6270\n",
            "Epoch 64/250\n",
            "26624/26624 [==============================] - 20s 746us/sample - loss: 0.5153 - acc: 0.7911 - val_loss: 0.6924 - val_acc: 0.6592\n",
            "Epoch 65/250\n",
            "26624/26624 [==============================] - 20s 749us/sample - loss: 0.5092 - acc: 0.7980 - val_loss: 0.7122 - val_acc: 0.7012\n",
            "Epoch 66/250\n",
            "26624/26624 [==============================] - 20s 748us/sample - loss: 0.4924 - acc: 0.8048 - val_loss: 0.6573 - val_acc: 0.7080\n",
            "Epoch 67/250\n",
            "26624/26624 [==============================] - 20s 749us/sample - loss: 0.5006 - acc: 0.8040 - val_loss: 0.8011 - val_acc: 0.6299\n",
            "Epoch 68/250\n",
            "26624/26624 [==============================] - 20s 751us/sample - loss: 0.5083 - acc: 0.8001 - val_loss: 0.9005 - val_acc: 0.6064\n",
            "Epoch 69/250\n",
            "26624/26624 [==============================] - 20s 751us/sample - loss: 0.4949 - acc: 0.8078 - val_loss: 0.7162 - val_acc: 0.6748\n",
            "Epoch 70/250\n",
            "26624/26624 [==============================] - 20s 750us/sample - loss: 0.4896 - acc: 0.8134 - val_loss: 0.8701 - val_acc: 0.6299\n",
            "Epoch 71/250\n",
            "26624/26624 [==============================] - 20s 750us/sample - loss: 0.4729 - acc: 0.8258 - val_loss: 0.8052 - val_acc: 0.6494\n",
            "Epoch 72/250\n",
            "26624/26624 [==============================] - 20s 745us/sample - loss: 0.4586 - acc: 0.8313 - val_loss: 0.7359 - val_acc: 0.6709\n",
            "Epoch 73/250\n",
            "26624/26624 [==============================] - 20s 746us/sample - loss: 0.4613 - acc: 0.8293 - val_loss: 0.8577 - val_acc: 0.6680\n",
            "Epoch 74/250\n",
            "26624/26624 [==============================] - 20s 744us/sample - loss: 0.4559 - acc: 0.8358 - val_loss: 0.7635 - val_acc: 0.6826\n",
            "Epoch 75/250\n",
            "26624/26624 [==============================] - 20s 744us/sample - loss: 0.4762 - acc: 0.8253 - val_loss: 0.6401 - val_acc: 0.7461\n",
            "Epoch 76/250\n",
            "26624/26624 [==============================] - 20s 749us/sample - loss: 0.4503 - acc: 0.8418 - val_loss: 0.6138 - val_acc: 0.7598\n",
            "Epoch 77/250\n",
            "26624/26624 [==============================] - 20s 746us/sample - loss: 0.4380 - acc: 0.8503 - val_loss: 0.6362 - val_acc: 0.7500\n",
            "Epoch 78/250\n",
            "26624/26624 [==============================] - 20s 749us/sample - loss: 0.4395 - acc: 0.8482 - val_loss: 0.7507 - val_acc: 0.7100\n",
            "Epoch 79/250\n",
            "26624/26624 [==============================] - 20s 752us/sample - loss: 0.4553 - acc: 0.8386 - val_loss: 0.6962 - val_acc: 0.7207\n",
            "Epoch 80/250\n",
            "26624/26624 [==============================] - 20s 744us/sample - loss: 0.4366 - acc: 0.8546 - val_loss: 0.7326 - val_acc: 0.7100\n",
            "Epoch 81/250\n",
            "26624/26624 [==============================] - 20s 749us/sample - loss: 0.4127 - acc: 0.8672 - val_loss: 0.8261 - val_acc: 0.6680\n",
            "Epoch 82/250\n",
            "26624/26624 [==============================] - 20s 747us/sample - loss: 0.4269 - acc: 0.8579 - val_loss: 0.5649 - val_acc: 0.7783\n",
            "Epoch 83/250\n",
            "26624/26624 [==============================] - 20s 746us/sample - loss: 0.4081 - acc: 0.8713 - val_loss: 0.5918 - val_acc: 0.7539\n",
            "Epoch 84/250\n",
            "26624/26624 [==============================] - 20s 750us/sample - loss: 0.4011 - acc: 0.8725 - val_loss: 0.7627 - val_acc: 0.7314\n",
            "Epoch 85/250\n",
            "26624/26624 [==============================] - 20s 743us/sample - loss: 0.3932 - acc: 0.8777 - val_loss: 0.7921 - val_acc: 0.6797\n",
            "Epoch 86/250\n",
            "26624/26624 [==============================] - 20s 750us/sample - loss: 0.3922 - acc: 0.8806 - val_loss: 0.9332 - val_acc: 0.6719\n",
            "Epoch 87/250\n",
            "26624/26624 [==============================] - 20s 749us/sample - loss: 0.3880 - acc: 0.8834 - val_loss: 0.6900 - val_acc: 0.7754\n",
            "Epoch 88/250\n",
            "26624/26624 [==============================] - 20s 749us/sample - loss: 0.3617 - acc: 0.8994 - val_loss: 0.8671 - val_acc: 0.7217\n",
            "Epoch 89/250\n",
            "26624/26624 [==============================] - 20s 744us/sample - loss: 0.3923 - acc: 0.8828 - val_loss: 0.7118 - val_acc: 0.7236\n",
            "Epoch 90/250\n",
            "26624/26624 [==============================] - 20s 750us/sample - loss: 0.3829 - acc: 0.8885 - val_loss: 0.7099 - val_acc: 0.7559\n",
            "Epoch 91/250\n",
            "26624/26624 [==============================] - 20s 750us/sample - loss: 0.3577 - acc: 0.9052 - val_loss: 0.6212 - val_acc: 0.7910\n",
            "Epoch 92/250\n",
            "26624/26624 [==============================] - 20s 750us/sample - loss: 0.3486 - acc: 0.9069 - val_loss: 0.6082 - val_acc: 0.7900\n",
            "Epoch 93/250\n",
            "26624/26624 [==============================] - 20s 747us/sample - loss: 0.3332 - acc: 0.9153 - val_loss: 0.5552 - val_acc: 0.8096\n",
            "Epoch 94/250\n",
            "26624/26624 [==============================] - 20s 750us/sample - loss: 0.3638 - acc: 0.8984 - val_loss: 0.6944 - val_acc: 0.7686\n",
            "Epoch 95/250\n",
            "26624/26624 [==============================] - 20s 749us/sample - loss: 0.3468 - acc: 0.9076 - val_loss: 0.8777 - val_acc: 0.7178\n",
            "Epoch 96/250\n",
            "26624/26624 [==============================] - 20s 752us/sample - loss: 0.3260 - acc: 0.9203 - val_loss: 0.4959 - val_acc: 0.8525\n",
            "Epoch 97/250\n",
            "26624/26624 [==============================] - 20s 744us/sample - loss: 0.2918 - acc: 0.9377 - val_loss: 0.7608 - val_acc: 0.7646\n",
            "Epoch 98/250\n",
            "26624/26624 [==============================] - 20s 748us/sample - loss: 0.2970 - acc: 0.9316 - val_loss: 0.6414 - val_acc: 0.7783\n",
            "Epoch 99/250\n",
            "26624/26624 [==============================] - 20s 744us/sample - loss: 0.3049 - acc: 0.9299 - val_loss: 0.6743 - val_acc: 0.7871\n",
            "Epoch 100/250\n",
            "26624/26624 [==============================] - 20s 748us/sample - loss: 0.3129 - acc: 0.9257 - val_loss: 0.5772 - val_acc: 0.7949\n",
            "Epoch 101/250\n",
            "26624/26624 [==============================] - 20s 749us/sample - loss: 0.3030 - acc: 0.9305 - val_loss: 0.5739 - val_acc: 0.8311\n",
            "Epoch 102/250\n",
            "26624/26624 [==============================] - 20s 747us/sample - loss: 0.3093 - acc: 0.9296 - val_loss: 0.4892 - val_acc: 0.8516\n",
            "Epoch 103/250\n",
            "26624/26624 [==============================] - 20s 748us/sample - loss: 0.2845 - acc: 0.9422 - val_loss: 0.5728 - val_acc: 0.7959\n",
            "Epoch 104/250\n",
            "26624/26624 [==============================] - 20s 742us/sample - loss: 0.2795 - acc: 0.9432 - val_loss: 0.5525 - val_acc: 0.8223\n",
            "Epoch 105/250\n",
            "26624/26624 [==============================] - 20s 750us/sample - loss: 0.2733 - acc: 0.9443 - val_loss: 0.4489 - val_acc: 0.8652\n",
            "Epoch 106/250\n",
            "26624/26624 [==============================] - 20s 750us/sample - loss: 0.2734 - acc: 0.9447 - val_loss: 0.4217 - val_acc: 0.8896\n",
            "Epoch 107/250\n",
            "26624/26624 [==============================] - 20s 750us/sample - loss: 0.2525 - acc: 0.9567 - val_loss: 0.5270 - val_acc: 0.8271\n",
            "Epoch 108/250\n",
            "26624/26624 [==============================] - 20s 750us/sample - loss: 0.2671 - acc: 0.9461 - val_loss: 0.4515 - val_acc: 0.8652\n",
            "Epoch 109/250\n",
            "26624/26624 [==============================] - 20s 752us/sample - loss: 0.3025 - acc: 0.9325 - val_loss: 0.7330 - val_acc: 0.7725\n",
            "Epoch 110/250\n",
            "26624/26624 [==============================] - 20s 749us/sample - loss: 0.3042 - acc: 0.9339 - val_loss: 0.4407 - val_acc: 0.8896\n",
            "Epoch 111/250\n",
            "26624/26624 [==============================] - 20s 753us/sample - loss: 0.2493 - acc: 0.9611 - val_loss: 0.5090 - val_acc: 0.8496\n",
            "Epoch 112/250\n",
            "26624/26624 [==============================] - 20s 751us/sample - loss: 0.2468 - acc: 0.9571 - val_loss: 0.4701 - val_acc: 0.8652\n",
            "Epoch 113/250\n",
            "26624/26624 [==============================] - 20s 742us/sample - loss: 0.2566 - acc: 0.9536 - val_loss: 0.4743 - val_acc: 0.8594\n",
            "Epoch 114/250\n",
            "26624/26624 [==============================] - 20s 744us/sample - loss: 0.2663 - acc: 0.9494 - val_loss: 0.5781 - val_acc: 0.8320\n",
            "Epoch 115/250\n",
            "26624/26624 [==============================] - 20s 750us/sample - loss: 0.2560 - acc: 0.9550 - val_loss: 0.6839 - val_acc: 0.7812\n",
            "Epoch 116/250\n",
            "26624/26624 [==============================] - 20s 754us/sample - loss: 0.2255 - acc: 0.9702 - val_loss: 0.5022 - val_acc: 0.8613\n",
            "Epoch 117/250\n",
            "26624/26624 [==============================] - 20s 748us/sample - loss: 0.2172 - acc: 0.9703 - val_loss: 0.3685 - val_acc: 0.8955\n",
            "Epoch 118/250\n",
            "26624/26624 [==============================] - 20s 748us/sample - loss: 0.2213 - acc: 0.9668 - val_loss: 0.4281 - val_acc: 0.8750\n",
            "Epoch 119/250\n",
            "26624/26624 [==============================] - 20s 751us/sample - loss: 0.2218 - acc: 0.9666 - val_loss: 0.4061 - val_acc: 0.8672\n",
            "Epoch 120/250\n",
            "26624/26624 [==============================] - 20s 747us/sample - loss: 0.2357 - acc: 0.9590 - val_loss: 0.7073 - val_acc: 0.7939\n",
            "Epoch 121/250\n",
            "26624/26624 [==============================] - 20s 746us/sample - loss: 0.2419 - acc: 0.9601 - val_loss: 0.7181 - val_acc: 0.7822\n",
            "Epoch 122/250\n",
            "26624/26624 [==============================] - 20s 749us/sample - loss: 0.2282 - acc: 0.9667 - val_loss: 0.3774 - val_acc: 0.8965\n",
            "Epoch 123/250\n",
            "26624/26624 [==============================] - 20s 749us/sample - loss: 0.2033 - acc: 0.9783 - val_loss: 0.3085 - val_acc: 0.9297\n",
            "Epoch 124/250\n",
            "26624/26624 [==============================] - 20s 746us/sample - loss: 0.1960 - acc: 0.9768 - val_loss: 0.9362 - val_acc: 0.7441\n",
            "Epoch 125/250\n",
            "26624/26624 [==============================] - 20s 749us/sample - loss: 0.2405 - acc: 0.9563 - val_loss: 0.8510 - val_acc: 0.7568\n",
            "Epoch 126/250\n",
            "26624/26624 [==============================] - 20s 743us/sample - loss: 0.2624 - acc: 0.9505 - val_loss: 0.3793 - val_acc: 0.9072\n",
            "Epoch 127/250\n",
            "26624/26624 [==============================] - 20s 751us/sample - loss: 0.2277 - acc: 0.9671 - val_loss: 0.3526 - val_acc: 0.9014\n",
            "Epoch 128/250\n",
            "26624/26624 [==============================] - 20s 748us/sample - loss: 0.2168 - acc: 0.9724 - val_loss: 0.4932 - val_acc: 0.8799\n",
            "Epoch 129/250\n",
            "26624/26624 [==============================] - 20s 743us/sample - loss: 0.2086 - acc: 0.9748 - val_loss: 0.3836 - val_acc: 0.8936\n",
            "Epoch 130/250\n",
            "26624/26624 [==============================] - 20s 743us/sample - loss: 0.2125 - acc: 0.9710 - val_loss: 0.3110 - val_acc: 0.9336\n",
            "Epoch 131/250\n",
            "26624/26624 [==============================] - 20s 750us/sample - loss: 0.2104 - acc: 0.9721 - val_loss: 0.3457 - val_acc: 0.9268\n",
            "Epoch 132/250\n",
            "26624/26624 [==============================] - 20s 751us/sample - loss: 0.2009 - acc: 0.9770 - val_loss: 0.3019 - val_acc: 0.9287\n",
            "Epoch 133/250\n",
            "26624/26624 [==============================] - 20s 746us/sample - loss: 0.1807 - acc: 0.9843 - val_loss: 0.2779 - val_acc: 0.9395\n",
            "Epoch 134/250\n",
            "26624/26624 [==============================] - 20s 751us/sample - loss: 0.1726 - acc: 0.9854 - val_loss: 0.3733 - val_acc: 0.9014\n",
            "Epoch 135/250\n",
            "26624/26624 [==============================] - 20s 743us/sample - loss: 0.1700 - acc: 0.9855 - val_loss: 0.4033 - val_acc: 0.8975\n",
            "Epoch 136/250\n",
            "26624/26624 [==============================] - 20s 751us/sample - loss: 0.1876 - acc: 0.9768 - val_loss: 0.6346 - val_acc: 0.8330\n",
            "Epoch 137/250\n",
            "26624/26624 [==============================] - 20s 742us/sample - loss: 1.0005 - acc: 0.7314 - val_loss: 8.7836 - val_acc: 0.4883\n",
            "Epoch 138/250\n",
            "26624/26624 [==============================] - 20s 746us/sample - loss: 1.2759 - acc: 0.6061 - val_loss: 8.8348 - val_acc: 0.4883\n",
            "Epoch 139/250\n",
            "26624/26624 [==============================] - 20s 746us/sample - loss: 1.1511 - acc: 0.6233 - val_loss: 4.9506 - val_acc: 0.4883\n",
            "Epoch 140/250\n",
            "26624/26624 [==============================] - 20s 745us/sample - loss: 1.0172 - acc: 0.6425 - val_loss: 1.6400 - val_acc: 0.5225\n",
            "Epoch 141/250\n",
            "26624/26624 [==============================] - 20s 744us/sample - loss: 0.9413 - acc: 0.6550 - val_loss: 0.9534 - val_acc: 0.6191\n",
            "Epoch 142/250\n",
            "26624/26624 [==============================] - 20s 748us/sample - loss: 0.8923 - acc: 0.6600 - val_loss: 0.9086 - val_acc: 0.6221\n",
            "Epoch 143/250\n",
            "26624/26624 [==============================] - 20s 744us/sample - loss: 0.8520 - acc: 0.6726 - val_loss: 0.9086 - val_acc: 0.5859\n",
            "Epoch 144/250\n",
            "26624/26624 [==============================] - 20s 747us/sample - loss: 0.8311 - acc: 0.6694 - val_loss: 0.8745 - val_acc: 0.6406\n",
            "Epoch 145/250\n",
            "26624/26624 [==============================] - 20s 752us/sample - loss: 0.8054 - acc: 0.6823 - val_loss: 0.8778 - val_acc: 0.6182\n",
            "Epoch 146/250\n",
            "26624/26624 [==============================] - 20s 749us/sample - loss: 0.7763 - acc: 0.6911 - val_loss: 0.8545 - val_acc: 0.6270\n",
            "Epoch 147/250\n",
            "26624/26624 [==============================] - 20s 746us/sample - loss: 0.7614 - acc: 0.6959 - val_loss: 0.8419 - val_acc: 0.6494\n",
            "Epoch 148/250\n",
            "26624/26624 [==============================] - 20s 749us/sample - loss: 0.7395 - acc: 0.7030 - val_loss: 0.8545 - val_acc: 0.5127\n",
            "Epoch 149/250\n",
            "26624/26624 [==============================] - 20s 750us/sample - loss: 0.7361 - acc: 0.7006 - val_loss: 0.8666 - val_acc: 0.4883\n",
            "Epoch 150/250\n",
            "26624/26624 [==============================] - 20s 746us/sample - loss: 0.7150 - acc: 0.7121 - val_loss: 0.8324 - val_acc: 0.5371\n",
            "Epoch 151/250\n",
            "26624/26624 [==============================] - 20s 749us/sample - loss: 0.6958 - acc: 0.7205 - val_loss: 0.8685 - val_acc: 0.4883\n",
            "Epoch 152/250\n",
            "26624/26624 [==============================] - 20s 748us/sample - loss: 0.6931 - acc: 0.7229 - val_loss: 0.8625 - val_acc: 0.4883\n",
            "Epoch 153/250\n",
            "26624/26624 [==============================] - 20s 748us/sample - loss: 0.6737 - acc: 0.7334 - val_loss: 0.8391 - val_acc: 0.5244\n",
            "Epoch 154/250\n",
            "26624/26624 [==============================] - 20s 749us/sample - loss: 0.6733 - acc: 0.7288 - val_loss: 0.9291 - val_acc: 0.4883\n",
            "Epoch 155/250\n",
            "26624/26624 [==============================] - 20s 748us/sample - loss: 0.6542 - acc: 0.7452 - val_loss: 0.8722 - val_acc: 0.4854\n",
            "Epoch 156/250\n",
            "26624/26624 [==============================] - 20s 745us/sample - loss: 0.6417 - acc: 0.7532 - val_loss: 0.8626 - val_acc: 0.5186\n",
            "Epoch 157/250\n",
            "26624/26624 [==============================] - 20s 747us/sample - loss: 0.6367 - acc: 0.7548 - val_loss: 0.8451 - val_acc: 0.5342\n",
            "Epoch 158/250\n",
            "26624/26624 [==============================] - 20s 749us/sample - loss: 0.6053 - acc: 0.7734 - val_loss: 0.9151 - val_acc: 0.5088\n",
            "Epoch 159/250\n",
            "26624/26624 [==============================] - 20s 751us/sample - loss: 0.5869 - acc: 0.7856 - val_loss: 0.8846 - val_acc: 0.5244\n",
            "Epoch 160/250\n",
            "26624/26624 [==============================] - 20s 744us/sample - loss: 0.6070 - acc: 0.7700 - val_loss: 0.8989 - val_acc: 0.5400\n",
            "Epoch 161/250\n",
            "26624/26624 [==============================] - 20s 744us/sample - loss: 0.5608 - acc: 0.8036 - val_loss: 0.8720 - val_acc: 0.5723\n",
            "Epoch 162/250\n",
            "26624/26624 [==============================] - 20s 748us/sample - loss: 0.5498 - acc: 0.8077 - val_loss: 0.7836 - val_acc: 0.6221\n",
            "Epoch 163/250\n",
            "26624/26624 [==============================] - 20s 751us/sample - loss: 0.5367 - acc: 0.8144 - val_loss: 0.7860 - val_acc: 0.6328\n",
            "Epoch 164/250\n",
            "26624/26624 [==============================] - 20s 750us/sample - loss: 0.5021 - acc: 0.8336 - val_loss: 0.8852 - val_acc: 0.5566\n",
            "Epoch 165/250\n",
            "26624/26624 [==============================] - 20s 745us/sample - loss: 0.5083 - acc: 0.8336 - val_loss: 0.8175 - val_acc: 0.6006\n",
            "Epoch 166/250\n",
            "26624/26624 [==============================] - 20s 746us/sample - loss: 0.5090 - acc: 0.8305 - val_loss: 0.9710 - val_acc: 0.5410\n",
            "Epoch 167/250\n",
            "26624/26624 [==============================] - 20s 746us/sample - loss: 0.4703 - acc: 0.8543 - val_loss: 0.9444 - val_acc: 0.5977\n",
            "Epoch 168/250\n",
            "26624/26624 [==============================] - 20s 742us/sample - loss: 0.4659 - acc: 0.8569 - val_loss: 0.7895 - val_acc: 0.6445\n",
            "Epoch 169/250\n",
            "26624/26624 [==============================] - 20s 749us/sample - loss: 0.4667 - acc: 0.8566 - val_loss: 0.9411 - val_acc: 0.6162\n",
            "Epoch 170/250\n",
            "26624/26624 [==============================] - 20s 748us/sample - loss: 0.4242 - acc: 0.8773 - val_loss: 1.3769 - val_acc: 0.5498\n",
            "Epoch 171/250\n",
            "26624/26624 [==============================] - 20s 745us/sample - loss: 0.4591 - acc: 0.8590 - val_loss: 0.8849 - val_acc: 0.6338\n",
            "Epoch 172/250\n",
            "26624/26624 [==============================] - 20s 747us/sample - loss: 0.4503 - acc: 0.8690 - val_loss: 0.9358 - val_acc: 0.6113\n",
            "Epoch 173/250\n",
            "13312/26624 [==============>...............] - ETA: 9s - loss: 0.4215 - acc: 0.8821 "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzx4-kyYbGfI",
        "colab_type": "text"
      },
      "source": [
        "### Save Model Weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1iTz86KbNed",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_weights(os.path.join(PARAMS.MODEL_DIR,'{}_final.h5'.format(PARAMS.MODEL_NAME)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1M9sjHIbSnf",
        "colab_type": "text"
      },
      "source": [
        "### Plot Training Histoty"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndNSS7XIbXaK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('LOSS History')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.savefig(os.path.join(PARAMS.MODEL_DIR,'{}_history.png'.format(PARAMS.MODEL_NAME)))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}